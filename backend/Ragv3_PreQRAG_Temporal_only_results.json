[
    {
        "question": "What does the lecturer emphasize as more important than exact running time when analyzing algorithms in Lecture3_Sc1007?",
        "ground_truth": "The order of growth (number of operations). (mentioned in 0:06:27)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, Operation (us), n!, Input Size, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, HIH\n[Transcript] (05:32.85) Thanks.\n(05:33.17) I think there's a delay today.\n(05:35.33) The delay seems quite unusually high.\n(05:39.53) It takes about 5 to 6 seconds.\n(05:44.01) I will go through the slides, so if you have any questions, you can post them in the chat, and I will answer them later. Because of the delay, I may not be able to answer your questions in real-time.\n(05:59.28) By the time I'm discussing the next slide, you may still be seeing the previous slide.\n(06:05.80) Therefore, I may answer all of your questions at the end of the lecture.\n(06:14.52) That will also make it easier for me to edit later.\n(06:19.64) In the previous week, I mentioned the analysis of algorithms.\n(06:27.16) Why do we need to analyze algorithms?\n(06:29.88) You will find that when you analyze any algorithm, we are not really interested in the exact time you take because we don't know the specifications of the computer you are using.\n(06:53.20) We don't know how much time each primitive operation will take.\n(06:58.92) Instead, we are more interested in the order of growth, specifically how many operations you need to execute in this algorithm.\n(07:07.28) We are interested in the order of the number of operations.\n(07:11.88) The order is important.\n(07:14.36) In this diagram, you can see that if your algorithm has a number of operations that is N factorial, I don't care how many constants are involved; once it is an N factorial algorithm, the order of growth will cause your problem size to increase significantly.\n(07:35.72) The operations you need to take will grow exponentially very fast.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (37:34.08) You can also set that FN is in Big O.\n(37:36.76) It's not wrong.\n(37:38.24) Yeah.\n(37:43.32) Then the big, if it's infinity, then you can only set FN is in Big Omega GN.\n(37:54.24) But OK.\n(37:54.88) What I would like to highlight is that if it's a constant, the best answer should be this because it gives you the most precise representation. It tells people if your answer is C, then they know that they have the same growth rate.\n(38:14.24) Of course, it's not wrong to say it's Big O or Big Omega, but when you tell people it's Big O or Big Omega, they may not be sure whether you are talking about this algorithm in this case or that case.\n(38:27.60) Your representation may not accurately convey the actual situation.\n(38:33.88) So I would not say it's wrong, but this will be a better and more precise answer if you get the limit as a constant and a positive constant number.\n(38:47.72) It's better to set it in Big Theta GN. OK, so we move on to an example.\n(38:59.32) You have learned the English.\n(39:01.04) When you do the linked list, the time complexity of the sequential search is a kind of sequential search.\n(39:11.64) So what is the time complexity when you are trying to search a linked list?\n(39:18.08) You need to search all the elements in the linked list, and you need to check each of them once.\n(39:27.44) You are just trying to check whether the key is in the first item, whether the key is in the second item, or whether the key is in the third item.\n(39:40.12) Of course, it is not surprising that every time you may get a different number of searches.\n(39:47.40) Sometimes you are lucky and just need one search.\n(39:50.08) Sometimes you are unlucky, and you search through the whole list and still cannot find the answer because the key is not inside.\n(39:56.92) In such a situation, you expect to analyze the time complexity. You want to know the best case, worst case, and average case because your time complexity can vary.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, 23, 24, 25, 26, 27, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, ge, Operation (us), n!, Input Size, OF, TER SCIE, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, Next, Previous, Last Viewed, See All Slides, Zoom In, Custom Show, Show Presenter View, Screen, Pointer Options, Start Subtitles, Subtitle Settings, Camera, Keep Slides Updated, Update Slides, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, Record Share, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, 13n is linear, 13log2n is the slowest, 102 can be ignored when n is large, 13n2 and 130n2 have similar growth., 130n2 slightly faster, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, P Lecture_01_Introduction ToAlgo ..., PLecture_02_AnalysisOfAlgorith ...\n[Transcript] (02:10.60) OK, I think now you can hear me.\n(02:17.44) Hello, good afternoon.\n(02:55.25) Now we have 14 people, so maybe we should wait a while.\n(03:08.37) I will start the class at 3:30 today.\n(03:12.29) In today's lecture, I will try to complete the analysis of algorithms.\n(03:21.88) If possible, I will also try to cover the sequential search and binary search.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle-End\n[Visual labels] human face\n[Transcript] (48:04.20) Why not simply tell me it's \u0398(N) for a more precise representation?\n(48:09.36) So that is always where people get confused. If today I'm talking about the time complexity of sequential search, I didn't ask you for the best case, worst case, or average case.\n(48:20.76) So, yeah, sometimes constant, sometimes linear.\n(48:25.24) If I just want you to use one representation to represent the whole algorithm, the time complexity of this sequential search, then sometimes constant, sometimes linear. The best representation you can use is O(N).\n(48:43.52) But if I'm talking about a particular scenario, and this particular scenario is definitely the worst case, then I think it's better to use \u0398(N). Although it's not wrong to set O(N).\n(48:58.24) OK, so that is the time complexity of sequential search.\n(49:02.92) Not yet.\n(49:03.44) I think I only talked about when the key is in the list.\n(49:07.60) Someone asked what happens when the key is not in the list.\n(49:12.16) If the search key A is not in the list, then you need to search all the way to the end, and then you need to run one more iteration to end the search.\n(49:26.56) So the worst case in that scenario is C1 plus N times C2.\n(49:33.04) So it's still linear.\n(49:37.80) Since the probability of the search key being in the list is unknown, we don't know.\n(49:45.32) Unless you know all the possible data you can put in the linked list, it's very hard to determine the probability.\n(50:02.52) Generally, if you need to know the probability of being in the list and not being in the list, if it's in the list, the average is this; if not in the list, you always need to search through the whole list.\n(50:22.96) It takes C1 plus N times C2.\n(50:25.84) In either case, this is a linear function. You can see this is a linear function; this one is also a linear function.",
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] person, human face\n[Transcript] (01:03:30.84) I need to move this plate.\n(01:03:32.48) I need to move this plate.\n(01:03:33.44) I need to move this plate.\n(01:03:35.36) Then you can move from this tower to that tower.\n(01:03:38.12) You'll find that the number of moves is in 2^n.\n(01:03:44.72) Yeah.\n(01:03:44.96) So this is the Tower of Hanoi.\n(01:03:47.64) And the traveling salesman problem is something in n factorial.\n(01:03:54.24) So when the time complexity of algorithm A grows faster than algorithm B for the same problem, we say A is inferior to B. OK, then the space complexity.\n(01:04:13.76) We don't discuss too much about space complexity because usually, you just need to look at how many variables you declare, how many arrays you declare, how many linked lists, and how many things you store in the linked list, stack, or tree.\n(01:04:28.28) You will know how much space you use.\n(01:04:30.84) So then you can represent it. If your problem size is N and you need to use two times, then it's 2N.\n(01:04:59.76) If your problem size is N but you need to form a matrix N by N, then your space complexity will become N^2.\n(01:05:05.20) So that is how space complexity analysis will be similar to time complexity.\n(01:05:06.12) Time complexity.\n(01:05:06.12) You really need to look at the whole algorithm and figure out how many while loops you use, whether you use any if-else statements, whether you need to consider the best case, worst case, average case, and whether you use any recursive functions.\n(01:05:26.40) If you use a recursive function, you may need to write the recurrence equation and then solve it to find out the actual time complexity of that particular algorithm.\n(01:05:40.04) Yeah.\n(01:05:40.80) So sometimes, later on Wednesday, I will talk about the hash table. You will find that it is a very typical example of the space-time trade-off.\n(01:05:52.48) Because when we try to improve our algorithm to a certain point, we can't come up with any further improvements.\n(01:06:00.08) Some people may try to use more space, store more things, store more information, hard code all the possible cases, and store them in a lookup table or something like that.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, 13, 102, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, Operation (us), n!, Input Size, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, 13n is linear, 13log2n is the slowest, 102 can be ignored when n is large, 13n2 and 130n2 have similar growth., \u00b7 130n2 slightly faster, /13n2 + 102, 13n2 + 10\n[Transcript] (07:48.08) If your algorithm has a number of operations in this order, you will find that initially, it is quite slow.\n(07:56.68) However, as your problem size grows, it becomes very concerning.\n(08:02.96) Just a little increase beyond 10, and you will already reach a significant position compared to 13 log N, which represents linear and logarithmic functions.\n(08:22.64) The growth of these two is very concerning.\n(08:26.04) Therefore, we only need to worry about the order of growth.\n(08:30.04) It is sufficient when we analyze the algorithm.\n(08:34.24) We also mentioned that constants can often be ignored. For example, 13 N squared and 13 N squared plus 100 may initially differ, but when your problem size grows beyond 1000, the plus 100 becomes negligible.\n(08:55.92) You will also find that comparing 130 N squared and 13 N squared, yes, 130 N squared is 10 times more than 13 N squared.\n(09:10.96) However, when comparing the order of growth, the growth is about the same.\n(09:25.80) If you increase your problem size by 10 times, both algorithms will grow in a similar manner.\n(09:33.08) The way they grow is parallel.\n(09:36.64) So usually, we are not so concerned about whether it is 13 or 14 or 130."
        ],
        "answer": "In Lecture3_Sc1007, the lecturer emphasizes that the order of growth is more important than the exact running time when analyzing algorithms. They explain that we are not really interested in the exact time taken because it can vary based on the specifications of the computer being used. Instead, the focus should be on how many operations need to be executed in the algorithm, which is crucial for understanding its efficiency as the problem size increases. This concept is highlighted when the lecturer states, \"we are more interested in the order of growth, specifically how many operations you need to execute in this algorithm\" and \"the order is important\" at [06:29.88] and [07:07.28].",
        "context_precision": 0.81666666664625,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.8938643860871031,
        "question_type": "SINGLE_DOC",
        "time_taken": 66.74886059761047,
        "question_index": 1
    },
    {
        "question": "What example mentioned in Lecture3_Sc1007 is used to explain why factorial time complexity is impractical?",
        "ground_truth": "An algorithm with N! operations grows exponentially fast, making it infeasible. (mentioned in 0:07:07)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] person, human face\n[Transcript] (01:03:30.84) I need to move this plate.\n(01:03:32.48) I need to move this plate.\n(01:03:33.44) I need to move this plate.\n(01:03:35.36) Then you can move from this tower to that tower.\n(01:03:38.12) You'll find that the number of moves is in 2^n.\n(01:03:44.72) Yeah.\n(01:03:44.96) So this is the Tower of Hanoi.\n(01:03:47.64) And the traveling salesman problem is something in n factorial.\n(01:03:54.24) So when the time complexity of algorithm A grows faster than algorithm B for the same problem, we say A is inferior to B. OK, then the space complexity.\n(01:04:13.76) We don't discuss too much about space complexity because usually, you just need to look at how many variables you declare, how many arrays you declare, how many linked lists, and how many things you store in the linked list, stack, or tree.\n(01:04:28.28) You will know how much space you use.\n(01:04:30.84) So then you can represent it. If your problem size is N and you need to use two times, then it's 2N.\n(01:04:59.76) If your problem size is N but you need to form a matrix N by N, then your space complexity will become N^2.\n(01:05:05.20) So that is how space complexity analysis will be similar to time complexity.\n(01:05:06.12) Time complexity.\n(01:05:06.12) You really need to look at the whole algorithm and figure out how many while loops you use, whether you use any if-else statements, whether you need to consider the best case, worst case, average case, and whether you use any recursive functions.\n(01:05:26.40) If you use a recursive function, you may need to write the recurrence equation and then solve it to find out the actual time complexity of that particular algorithm.\n(01:05:40.04) Yeah.\n(01:05:40.80) So sometimes, later on Wednesday, I will talk about the hash table. You will find that it is a very typical example of the space-time trade-off.\n(01:05:52.48) Because when we try to improve our algorithm to a certain point, we can't come up with any further improvements.\n(01:06:00.08) Some people may try to use more space, store more things, store more information, hard code all the possible cases, and store them in a lookup table or something like that.",
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] person, human face\n[Transcript] (55:43.16) There's a chance that when you derive, you make a mistake by simplifying too much.\n(55:52.04) So there are a few ways to simplify, some simplification rules for asymptotic analysis.\n(56:00.64) If your FN is in O(GN), where C is some constant greater than 0, then you can also say FN is equal to O(GN), so FN is in O(GN).\n(56:15.80) If FN is O(GN) and GN is big O(HN), then you can also say FN is big O(HN) because this one is bounded.\n(56:27.92) FN is bounded by GN by some constant number, and GN is bounded above by another HN. HN grows much higher, so it's not surprising you can make this conclusion.\n(56:44.44) For example, how to clarify that?\n(56:51.52) OK, so FN is 2N, GN is N^2, and HN is N^3.\n(57:04.20) Of course, you can set FN in O(GN). Of course, FN and GN are in O(HN), so FN is also in O(HN).\n(57:15.04) Another scenario is if you have two functions, F1 and F2, where F1 is in O(G1N) and F2 is in O(G2N). Then you run the first one and add the second one.\n(57:35.84) In the end, what is the time complexity of the first algorithm followed by the second algorithm?\n(57:42.68) The thing depends on which one is bigger. You can conclude that it is the maximum of G1 and G2.\n(57:51.16) So if the first algorithm is linear and the second algorithm is logarithmic, then the overall time complexity of this whole algorithm is linear. The logarithmic growth is too low compared to linear.\n(58:07.08) So this one can actually be negligible.\n(58:09.36) This means the second one can be negligible.\n(58:38.80) So overall, you need to run these two, then it's F1 plus F2, and you can also say it is equal to O(G1 times G2).",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] person, software, computer\n[OCR] ge, OF, TER SCIE, Dntu.edu.sg) Loke Yuan Ren (yrloke, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Algorithms, Hash Tables, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, O Find, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke\n[Transcript] (03:35.24) I will review a bit.\n(03:42.92) I believe you have learned this in the first half.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (40:07.16) Sometimes you are lucky, which is the best case I have told you.\n(40:16.32) You just need one search, and you get the answer.\n(40:19.40) So no doubt that the time complexity of the best case is constant.\n(40:26.48) You just need to search C1, OK, then you don't even need to enter this while loop.\n(40:32.16) You will go straight to the answer.\n(40:37.16) Then you don't need to carry on.\n(40:38.64) The computation time is just C1.\n(40:43.32) You only need to run a constant operation.\n(40:50.24) The time complexity of the best case scenario in sequential search is Big Theta 1.\n(41:01.40) OK, worst case.\n(41:02.80) What is the worst case?\n(41:04.12) Now I am assuming that the search key is always in the list.\n(41:08.12) You definitely need to search the list.\n(41:09.64) So what is the worst case?\n(41:25.80) You can expect that you need to search all the data.\n(41:34.36) If you want to search all the data, in this example, you need to search all of them.\n(41:41.84) In the first case, you don't need to enter.\n(41:44.72) Then in the second case, after checking, if the key is not inside, you need to run this part for N - 1 iterations.\n(41:55.52) Then you reach the last one.\n(41:57.44) Unfortunately, if the key is at the last position, then you find it.\n(42:01.68) So that should be the worst case.\n(42:03.56) This means I need to run C1 + (N - 1) * C2.\n(42:10.44) Yeah, because in this part, you need to run N - 1 iterations.\n(42:15.60) In short, in the end, this is a linear function.\n(42:19.44) I can conclude that in the worst case, the time complexity of the worst case scenario is Big Theta N.\n(42:42.24) For this kind of algorithm, you can see you now have the picture.\n(42:45.24) The best case is constant, and the worst case is linear.\n(42:53.56) We also would like to know, on average, how much time you need to take.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (37:34.08) You can also set that FN is in Big O.\n(37:36.76) It's not wrong.\n(37:38.24) Yeah.\n(37:43.32) Then the big, if it's infinity, then you can only set FN is in Big Omega GN.\n(37:54.24) But OK.\n(37:54.88) What I would like to highlight is that if it's a constant, the best answer should be this because it gives you the most precise representation. It tells people if your answer is C, then they know that they have the same growth rate.\n(38:14.24) Of course, it's not wrong to say it's Big O or Big Omega, but when you tell people it's Big O or Big Omega, they may not be sure whether you are talking about this algorithm in this case or that case.\n(38:27.60) Your representation may not accurately convey the actual situation.\n(38:33.88) So I would not say it's wrong, but this will be a better and more precise answer if you get the limit as a constant and a positive constant number.\n(38:47.72) It's better to set it in Big Theta GN. OK, so we move on to an example.\n(38:59.32) You have learned the English.\n(39:01.04) When you do the linked list, the time complexity of the sequential search is a kind of sequential search.\n(39:11.64) So what is the time complexity when you are trying to search a linked list?\n(39:18.08) You need to search all the elements in the linked list, and you need to check each of them once.\n(39:27.44) You are just trying to check whether the key is in the first item, whether the key is in the second item, or whether the key is in the third item.\n(39:40.12) Of course, it is not surprising that every time you may get a different number of searches.\n(39:47.40) Sometimes you are lucky and just need one search.\n(39:50.08) Sometimes you are unlucky, and you search through the whole list and still cannot find the answer because the key is not inside.\n(39:56.92) In such a situation, you expect to analyze the time complexity. You want to know the best case, worst case, and average case because your time complexity can vary.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] software, computer\n[OCR] ge, OF, TER SCIE, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke, \u00b7 Best-case complexity: o (1), 1 comparison against key (the first item is the search key), \u00b7 Worst-case complexity: o (n), n comparison against key (Either the last item or no item is the search key), \u00b7 Average-case complexity:, Key is in the search array:, e represents the event that the key appears in ith position, so its probability P(e,) = 1/n, T(ej) is the no. of comparisons done, Both worst and average complexity are (n), Key is not in the search array:, n comparisons (if it is an linked list, you may need to take an extra comparison[Transcript] (03:46.44) I will discuss the time complexity of these two algorithms, and then we will talk about the hash table."
        ],
        "answer": "In Lecture3_Sc1007, the example used to explain why factorial time complexity is impractical is the \"traveling salesman problem.\" This problem is noted for its complexity, which grows in factorial time, making it challenging to solve efficiently as the number of cities increases. This is highlighted at the timestamp \"Covered at 01:03:47.64.\"",
        "context_precision": 0.9999999999,
        "context_recall": 0.5,
        "faithfulness_result": 0.4,
        "response_relevancy": 0.9938829784601442,
        "question_type": "SINGLE_DOC",
        "time_taken": 73.66938996315002,
        "question_index": 2
    },
    {
        "question": "In Lecture3_Sc1007, why are constants ignored in asymptotic analysis?",
        "ground_truth": "Because for large input sizes, constants (like +100) become negligible compared to growth rate. (mentioned in 0:08:44)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, 13, 102, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, Operation (us), n!, Input Size, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, 13n is linear, 13log2n is the slowest, 102 can be ignored when n is large, 13n2 and 130n2 have similar growth., \u00b7 130n2 slightly faster, /13n2 + 102, 13n2 + 10\n[Transcript] (07:48.08) If your algorithm has a number of operations in this order, you will find that initially, it is quite slow.\n(07:56.68) However, as your problem size grows, it becomes very concerning.\n(08:02.96) Just a little increase beyond 10, and you will already reach a significant position compared to 13 log N, which represents linear and logarithmic functions.\n(08:22.64) The growth of these two is very concerning.\n(08:26.04) Therefore, we only need to worry about the order of growth.\n(08:30.04) It is sufficient when we analyze the algorithm.\n(08:34.24) We also mentioned that constants can often be ignored. For example, 13 N squared and 13 N squared plus 100 may initially differ, but when your problem size grows beyond 1000, the plus 100 becomes negligible.\n(08:55.92) You will also find that comparing 130 N squared and 13 N squared, yes, 130 N squared is 10 times more than 13 N squared.\n(09:10.96) However, when comparing the order of growth, the growth is about the same.\n(09:25.80) If you increase your problem size by 10 times, both algorithms will grow in a similar manner.\n(09:33.08) The way they grow is parallel.\n(09:36.64) So usually, we are not so concerned about whether it is 13 or 14 or 130.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] Dntu.edu.sg) Loke Yuan Ren (yrloke, In Ren (yrloke@ntu.edu.sg) Loke Y, Big-Oh Notation (O), Definition 3.1 O-notation: Let f and g be two functions such that f(n) : N - R+ and g(n) : N-+RR+,, f(n) is said to be in O(g(n)), denoted f(n) \u20ac 0(g(n)), if f(n) is bounded above by some constant multiple, of g(n) for all large n, i.e ., the set of functions can be defined as, f(n), f(n) = 4n + 3 and g(n) = n3, Let c = 5, n0 = 3, Vn \u22653, f(n)=0(g(n)), f(n)=4n+3, Vn \u2265 3, g(n), 4n +3 \u2264n3, lf f(n) = 0(g(n)), we say, g (n) is asymptotic upper bound of f (n), ( D), Big-Oh Notation (O) - Alternative definition, Let c = 1, n0 = 3, du.sa) L\n[Transcript] (22:48.28) If FN is equal to big O GN, then we can set that GN is the asymptotic upper bound of FN.\n(23:02.88) Another way you will find that sometimes it may not be easy to find a constant, or you can't find the constant N to fulfill the criteria.\n(23:17.92) There is another alternate definition, which is to let F and G be two functions such that FN is some integer raised to some positive real number, and GN is some integer raised to some positive real number.\n(23:36.56) Then we can take the limit.\n(23:39.12) We take the limit as N approaches infinity of FN over GN.\n(23:43.88) If GN grows much faster than FN or GN has the same growth rate as FN, then this ratio, when the limit as N approaches infinity, cannot go to infinity.\n(24:05.32) It's not possible to go to infinity because GN can grow faster or at the same rate, so this ratio cannot go to infinity.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] person, computer, human face\n[OCR] Dntu.edu.sg) Loke Yuan Ren (yrloke, Big-Oh Notation (O), of g(n) for all large n, i.e ., the set of functions can be defined as, f(n), f(n) = 4n + 3 and g(n) = n3, f(n)=0(g(n)), g(n), lf f(n) = 0(g(n)), we say, g (n) is asymptotic upper bound of f (n), Big-Oh Notation (O) - Alternative definition, Let c = 1, n0 = 3, lim, = lim, Definition 3.1 O-notation: Let f and g be two functions such that f(n) : N - R+ and g(n) : N-RR+,, n3, 4n + 3 \u2264n3, n->00 g (n), f(n) is said to be in O(g(n)), denoted f(n) E O(g(n)), if f(n) is bounded above by some constant multiple, f(n) = 4n+3\n[Transcript] (29:53.32) The complexity is constant.\n(29:54.64) Normally, we write O(1) instead of O(0); that is a bit weak.\n(29:59.88) But if you want to say that 0 is a constant, 1 is a constant, all these are constant numbers, then they should belong to the same category.\n(30:10.28) Where does the 5 come from again?\n(30:11.32) The 5 is defined again and again.\n(30:15.80) I think this is also my mistake.\n(30:18.52) Again, I think this is from the earlier slide.\n(30:28.76) In this case, N cubed is GN, so FN should be less than or equal to GN.\n(30:43.28) Yes.\n(30:45.32) So C can be a fraction; as long as it's a constant, you can give me a fraction, 0.1, 0.5, or 0.33333; that is fine as long as you fulfill that criterion based on the definition.",
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] person, human face\n[Transcript] (55:43.16) There's a chance that when you derive, you make a mistake by simplifying too much.\n(55:52.04) So there are a few ways to simplify, some simplification rules for asymptotic analysis.\n(56:00.64) If your FN is in O(GN), where C is some constant greater than 0, then you can also say FN is equal to O(GN), so FN is in O(GN).\n(56:15.80) If FN is O(GN) and GN is big O(HN), then you can also say FN is big O(HN) because this one is bounded.\n(56:27.92) FN is bounded by GN by some constant number, and GN is bounded above by another HN. HN grows much higher, so it's not surprising you can make this conclusion.\n(56:44.44) For example, how to clarify that?\n(56:51.52) OK, so FN is 2N, GN is N^2, and HN is N^3.\n(57:04.20) Of course, you can set FN in O(GN). Of course, FN and GN are in O(HN), so FN is also in O(HN).\n(57:15.04) Another scenario is if you have two functions, F1 and F2, where F1 is in O(G1N) and F2 is in O(G2N). Then you run the first one and add the second one.\n(57:35.84) In the end, what is the time complexity of the first algorithm followed by the second algorithm?\n(57:42.68) The thing depends on which one is bigger. You can conclude that it is the maximum of G1 and G2.\n(57:51.16) So if the first algorithm is linear and the second algorithm is logarithmic, then the overall time complexity of this whole algorithm is linear. The logarithmic growth is too low compared to linear.\n(58:07.08) So this one can actually be negligible.\n(58:09.36) This means the second one can be negligible.\n(58:38.80) So overall, you need to run these two, then it's F1 plus F2, and you can also say it is equal to O(G1 times G2).",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] person, computer, human face\n[Transcript] (34:33.92) As long as you can find such C1 and C2, and after a certain number, they are always bounded above and below by some constant multiple of GN, then you can conclude that FN is in Big Theta GN.\n(34:56.36) Another alternate definition is by taking the limit. In that case, because what does Big Theta mean?\n(35:07.52) The idea behind Big Theta is that FN and GN have the same growth rate.\n(35:12.88) If they have the same growth rate, then the ratio of FN to GN must be a constant.\n(35:17.20) It cannot be equal to 0.\n(35:18.92) It also cannot go to infinity.\n(35:21.88) So this C must fall between 0 and infinity; it cannot be equal to 0 or infinity.\n(35:30.52) It must be some positive constant. Then, if I can set FN equal to Big Theta GN, it implies that GN is the asymptotic lower bound of FN.\n(35:55.88) A lot of the time, I would say that the second definition is commonly used unless it's really hard to take the ratio. Sometimes these two functions are not polynomial functions.\n(36:13.72) Taking the limit is very difficult.\n(36:16.08) For example, N factorial compared to some logarithmic function is not easy.\n(36:27.92) Then you may use the first so-called original definition to find whether it's in Big Theta, Big Omega, or Big O of GN.\n(36:47.48) In summary, if you can claim that FN is in Big O GN, which means their ratio, FN over GN, as you take the limit, can either be 0 or a constant, but not 0 and not less than infinity.\n(37:20.84) So in fact, anything that falls in this category means that it is actually the tight bound.\n(37:28.36) The definition is Big Theta.\n(37:29.68) Once it's Big Theta, it is not surprising that you can set FN is also in Big Omega.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, Operation (us), n!, Input Size, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, 13n is linear, 13log2n is the slowest, 102 can be ignored when n is large, 13n2 and 130n2 have similar growth., 130n2 slightly faster, HIH, \u00b7 130n2 slightly faster, /13n2 + 102, Loke Yuan Ren (y, @ntu.edu.sg) Loke Yuan Ren (yrlol, 130n2 slig, Asymptotic Notations, notations used for describing the order of growth of a given function., \u00b7 Big-Oh ( O), Big-Omega ( 2 ) and Big-Theta ( O ) are asymptotic (set)\n[Transcript] (09:48.68) What we are concerned about is whether it is N squared, which is a quadratic function.\n(09:51.56) We compare whether it is a linear function or a quadratic function.\n(09:57.52) We are not so bothered about whether it is 13 or 130.\n(10:02.20) Of course, with 130, you would expect it to be about 10 times slower than 13 N squared.\n(10:07.80) However, I said \"slower\" because you need to take more time.\n(10:12.88) The growth is much faster as your problem size increases.\n(10:22.40) Therefore, people start thinking about how to represent the time complexity of my algorithm since the order of growth is more important.\n(10:35.44) People began to use a notation called asymptotic notation to represent time complexity.\n(10:43.60) All these notations, such as big O, big Omega, and big Theta, may also include small O, small Omega, and small Theta in some books.\n(10:51.88) Anyway, all of this is just a notation to represent time complexity and describe the order of growth."
        ],
        "answer": "In Lecture3_Sc1007, constants are ignored in asymptotic analysis because they become negligible as the problem size grows larger. The focus is on the order of growth rather than the specific values of constants. For example, when comparing functions like 13N\u00b2 and 13N\u00b2 + 100, the additional constant (100) becomes insignificant when the problem size exceeds a certain threshold, such as 1000. This means that for large inputs, the growth rates of these functions are similar, and the constants do not affect the overall analysis of the algorithm's efficiency. This concept is emphasized when it is stated that \"constants can often be ignored\" and that \"when your problem size grows beyond 1000, the plus 100 becomes negligible\" (Covered at 08:34).",
        "context_precision": 0.9999999999833333,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9571390277048345,
        "question_type": "SINGLE_DOC",
        "time_taken": 71.43140387535095,
        "question_index": 3
    },
    {
        "question": "What are the three asymptotic notations introduced by the lecturer in Lecture3_Sc1007?",
        "ground_truth": "Big O, Big Omega, and Big Theta. (mentioned in 0:11:31)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] person\n[OCR] @ntu.edu.sg) Loke Yuan Ren (yrlol, Asymptotic Notations, notations used for describing the order of growth of a given function., \u00b7 Big-Oh ( O), Big-Omega ( 2 ) and Big-Theta ( O ) are asymptotic (set), f \u20ac 0(g) Set of functions that grow at lower or, same rate as g, that grow at lower or, f E 0(g) Set of functions that grow at lower or\n[Transcript] (11:24.12) Here, we only introduce three notations: big O, big Theta, and big Omega.\n(11:31.24) What do they mean?\n(11:33.48) This representation is a set of functions.\n(11:35.48) If I mentioned last time that if my G(N) is N squared, then if I want to set whatever function is F, F is considered in big O if F is in big O of G.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 1000000, 100000, 10000, 1000, 100, 10, Order of Growth, Growth Rate Graph, 13n2, 13n2 + 102, Operation (us), n!, Input Size, Dntu.edu.sg) Loke Yuan Ren (yrloke, n! is the fastest growth, 2n is the second, 13n is linear, 13log2n is the slowest, 102 can be ignored when n is large, 13n2 and 130n2 have similar growth., 130n2 slightly faster, HIH, \u00b7 130n2 slightly faster, /13n2 + 102, Loke Yuan Ren (y, @ntu.edu.sg) Loke Yuan Ren (yrlol, 130n2 slig, Asymptotic Notations, notations used for describing the order of growth of a given function., \u00b7 Big-Oh ( O), Big-Omega ( 2 ) and Big-Theta ( O ) are asymptotic (set)\n[Transcript] (09:48.68) What we are concerned about is whether it is N squared, which is a quadratic function.\n(09:51.56) We compare whether it is a linear function or a quadratic function.\n(09:57.52) We are not so bothered about whether it is 13 or 130.\n(10:02.20) Of course, with 130, you would expect it to be about 10 times slower than 13 N squared.\n(10:07.80) However, I said \"slower\" because you need to take more time.\n(10:12.88) The growth is much faster as your problem size increases.\n(10:22.40) Therefore, people start thinking about how to represent the time complexity of my algorithm since the order of growth is more important.\n(10:35.44) People began to use a notation called asymptotic notation to represent time complexity.\n(10:43.60) All these notations, such as big O, big Omega, and big Theta, may also include small O, small Omega, and small Theta in some books.\n(10:51.88) Anyway, all of this is just a notation to represent time complexity and describe the order of growth.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 125, 71-734-14, Next, TER SCIEN, Asymptotic Notations, notations used for describing the order of growth of a given function., f \u20ac 0(g) Set of functions that grow at lower or, same rate as g, , germens ns - OneNote, Loke Yu, Insert Draw History Review Vie, To Do (Ctrl+ 1), Important (Ctrl+2), To Do Find Outlook Email Meeting, Tags, New Section 2, New Section 3 New Section, Thursday, August 18, 2022 7:02 PM, togs N = login, o. WCN) = It-in-18+ 2/0-2, W (N) * =NTWCNX) who 5, lice 3, or, line 102-104, ECA 0.5, Fen 05 -7 Loa = 0.25, 1+213+4+5 t .... Ette, Fata x, Jogo's, 71-73 4-14, germens ns, I.sg) Loke Yuan Ren (yrlo, Insert Draw History, Basic Text, Email Meetings, Loke's Notebook, Add Page, S = 20-615+20-262) +2 -- 3, ago, Best Case: 2=0.5, WC 13=0, Untitled page, MIICIA) - 1). + (inti) t ...., Insert Draw History Review View, W (0) = or, TE, New Section, f (n ) = n, GINEERIN, Quick Notes New Section 1, Loke's Notebook Quick Notes New Section 1, LT, Tag, brary, Quick Note, 3- 4, view View, L 125, Quick Nates New Section 1 New Section 2, New Section 2 New Section 3 New Section, k=1, Vie, New Section 5, Insert Draw, Quick Notes, CA -0.5 + LOA =0s, New Section 2 New Section 3, iew View, Loke's Notebook Quick Notes, SCSE[Transcript] ",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] 125, Next, TER SCIEN, Asymptotic Notations, notations used for describing the order of growth of a given function., f \u20ac 0(g) Set of functions that grow at lower or, same rate as g, , germens ns - OneNote, Loke Yu, Insert Draw History Review Vie, To Do (Ctrl+ 1), Important (Ctrl+2), To Do Find Outlook Email Meeting, Tags, New Section 2, New Section 3 New Section, Thursday, August 18, 2022 7:02 PM, togs N = login, o. WCN) = It-in-18+ 2/0-2, W (N) * =NTWCNX) who 5, lice 3, or, line 102-104, ECA 0.5, Fen 05 -7 Loa = 0.25, 1+213+4+5 t .... Ette, Fata x, Jogo's, 71-73 4-14, germens ns, I.sg) Loke Yuan Ren (yrlo, Insert Draw History, Basic Text, Email Meetings, Loke's Notebook, Add Page, S = 20-615+20-262) +2 -- 3, ago, Best Case: 2=0.5, WC 13=0, Untitled page, MIICIA) - 1). + (inti) t ...., Insert Draw History Review View, W (0) = or, TE, Loke's Notebook Quick Notes New Section 1 New Section 2, New Section, f (n ) = n, GINEERIN, Quick Notes New Section 1, Loke's Notebook Quick Notes New Section 1, LT, Tag, brary\n[Transcript] ",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[OCR] Dntu.edu.sg) Loke Yuan Ren (yrloke, In Ren (yrloke@ntu.edu.sg) Loke Y, Big-Oh Notation (O), Definition 3.1 O-notation: Let f and g be two functions such that f(n) : N - R+ and g(n) : N-+RR+,, f(n) is said to be in O(g(n)), denoted f(n) \u20ac 0(g(n)), if f(n) is bounded above by some constant multiple, of g(n) for all large n, i.e ., the set of functions can be defined as, f(n), f(n) = 4n + 3 and g(n) = n3, Let c = 5, n0 = 3, Vn \u22653, f(n)=0(g(n)), f(n)=4n+3, Vn \u2265 3, g(n), 4n +3 \u2264n3, lf f(n) = 0(g(n)), we say, g (n) is asymptotic upper bound of f (n), ( D), Big-Oh Notation (O) - Alternative definition, Let c = 1, n0 = 3, du.sa) L\n[Transcript] (22:48.28) If FN is equal to big O GN, then we can set that GN is the asymptotic upper bound of FN.\n(23:02.88) Another way you will find that sometimes it may not be easy to find a constant, or you can't find the constant N to fulfill the criteria.\n(23:17.92) There is another alternate definition, which is to let F and G be two functions such that FN is some integer raised to some positive real number, and GN is some integer raised to some positive real number.\n(23:36.56) Then we can take the limit.\n(23:39.12) We take the limit as N approaches infinity of FN over GN.\n(23:43.88) If GN grows much faster than FN or GN has the same growth rate as FN, then this ratio, when the limit as N approaches infinity, cannot go to infinity.\n(24:05.32) It's not possible to go to infinity because GN can grow faster or at the same rate, so this ratio cannot go to infinity.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] person, human face\n[OCR] Dntu.edu.sg) Loke Yuan Ren (yrloke, Big-Oh Notation (O), c g(n), of g(n) for all large n, i.e ., the set of functions can be defined as, f(n), no, f(n) = 4n + 3 and g(n) = n3, g(n), lf f(n) = 0(g(n)), we say, g (n) is asymptotic upper bound of f (n), Big-Oh Notation (O) - Alternative definition, Let c = 1, n0 = 3, lim, = lim, Definition 3.1 O-notation: Let f and g be two functions such that f(n) : N - R+ and g(n) : N-RR+,, of g(n) for all large n, i.e ., the set of functions can be defined as., n3, f(n) is said to be in 2(g(n)), denoted f(n) \u20ac 2(g(n)), if f(n) is bounded below by some constant multiple, Let c=1/5, no =0, f(n) \u2265 (1/5)g(n), 4n+3 \u2265 (1/5)5n, for all n20, 4n + 3 \u2264n3, f(n) is said to be in O(g(n)), denoted f(n) E O(g(n)), if f(n) is bounded above by some constant multiple, f(n) = 4n+3, Big-Omega Notation (Q), f(n) = 4n + 3 and g(n) = 5n\n[Transcript] (31:06.64) OK, so the same thing, not surprising.\n(31:09.20) The rest, the other two are playing the same matter.\n(31:14.12) Big Omega notation is bounded below.\n(31:18.44) This time it's bounded below by some constant multiplied by GN for some large N.\n(31:24.16) OK, I am trying to use another alternate definition."
        ],
        "answer": "The three asymptotic notations introduced by the lecturer in Lecture3_Sc1007 are Big O, Big Theta, and Big Omega. These notations are used to describe the order of growth of a given function. Specifically, Big O represents an upper bound, Big Theta represents a tight bound, and Big Omega represents a lower bound of the growth rate of functions. This is covered at [11:24].",
        "context_precision": 0.8333333333055556,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9937454549906857,
        "question_type": "SINGLE_DOC",
        "time_taken": 57.01918005943298,
        "question_index": 4
    },
    {
        "question": "What is the best-case time complexity of sequential search in a linked list in Lecture3_Sc1007?",
        "ground_truth": "Constant time, Big Theta(1). (mentioned in 0:40:07)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Middle-End\n[Visual labels] human face\n[Transcript] (48:04.20) Why not simply tell me it's \u0398(N) for a more precise representation?\n(48:09.36) So that is always where people get confused. If today I'm talking about the time complexity of sequential search, I didn't ask you for the best case, worst case, or average case.\n(48:20.76) So, yeah, sometimes constant, sometimes linear.\n(48:25.24) If I just want you to use one representation to represent the whole algorithm, the time complexity of this sequential search, then sometimes constant, sometimes linear. The best representation you can use is O(N).\n(48:43.52) But if I'm talking about a particular scenario, and this particular scenario is definitely the worst case, then I think it's better to use \u0398(N). Although it's not wrong to set O(N).\n(48:58.24) OK, so that is the time complexity of sequential search.\n(49:02.92) Not yet.\n(49:03.44) I think I only talked about when the key is in the list.\n(49:07.60) Someone asked what happens when the key is not in the list.\n(49:12.16) If the search key A is not in the list, then you need to search all the way to the end, and then you need to run one more iteration to end the search.\n(49:26.56) So the worst case in that scenario is C1 plus N times C2.\n(49:33.04) So it's still linear.\n(49:37.80) Since the probability of the search key being in the list is unknown, we don't know.\n(49:45.32) Unless you know all the possible data you can put in the linked list, it's very hard to determine the probability.\n(50:02.52) Generally, if you need to know the probability of being in the list and not being in the list, if it's in the list, the average is this; if not in the list, you always need to search through the whole list.\n(50:22.96) It takes C1 plus N times C2.\n(50:25.84) In either case, this is a linear function. You can see this is a linear function; this one is also a linear function.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (40:07.16) Sometimes you are lucky, which is the best case I have told you.\n(40:16.32) You just need one search, and you get the answer.\n(40:19.40) So no doubt that the time complexity of the best case is constant.\n(40:26.48) You just need to search C1, OK, then you don't even need to enter this while loop.\n(40:32.16) You will go straight to the answer.\n(40:37.16) Then you don't need to carry on.\n(40:38.64) The computation time is just C1.\n(40:43.32) You only need to run a constant operation.\n(40:50.24) The time complexity of the best case scenario in sequential search is Big Theta 1.\n(41:01.40) OK, worst case.\n(41:02.80) What is the worst case?\n(41:04.12) Now I am assuming that the search key is always in the list.\n(41:08.12) You definitely need to search the list.\n(41:09.64) So what is the worst case?\n(41:25.80) You can expect that you need to search all the data.\n(41:34.36) If you want to search all the data, in this example, you need to search all of them.\n(41:41.84) In the first case, you don't need to enter.\n(41:44.72) Then in the second case, after checking, if the key is not inside, you need to run this part for N - 1 iterations.\n(41:55.52) Then you reach the last one.\n(41:57.44) Unfortunately, if the key is at the last position, then you find it.\n(42:01.68) So that should be the worst case.\n(42:03.56) This means I need to run C1 + (N - 1) * C2.\n(42:10.44) Yeah, because in this part, you need to run N - 1 iterations.\n(42:15.60) In short, in the end, this is a linear function.\n(42:19.44) I can conclude that in the worst case, the time complexity of the worst case scenario is Big Theta N.\n(42:42.24) For this kind of algorithm, you can see you now have the picture.\n(42:45.24) The best case is constant, and the worst case is linear.\n(42:53.56) We also would like to know, on average, how much time you need to take.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (37:34.08) You can also set that FN is in Big O.\n(37:36.76) It's not wrong.\n(37:38.24) Yeah.\n(37:43.32) Then the big, if it's infinity, then you can only set FN is in Big Omega GN.\n(37:54.24) But OK.\n(37:54.88) What I would like to highlight is that if it's a constant, the best answer should be this because it gives you the most precise representation. It tells people if your answer is C, then they know that they have the same growth rate.\n(38:14.24) Of course, it's not wrong to say it's Big O or Big Omega, but when you tell people it's Big O or Big Omega, they may not be sure whether you are talking about this algorithm in this case or that case.\n(38:27.60) Your representation may not accurately convey the actual situation.\n(38:33.88) So I would not say it's wrong, but this will be a better and more precise answer if you get the limit as a constant and a positive constant number.\n(38:47.72) It's better to set it in Big Theta GN. OK, so we move on to an example.\n(38:59.32) You have learned the English.\n(39:01.04) When you do the linked list, the time complexity of the sequential search is a kind of sequential search.\n(39:11.64) So what is the time complexity when you are trying to search a linked list?\n(39:18.08) You need to search all the elements in the linked list, and you need to check each of them once.\n(39:27.44) You are just trying to check whether the key is in the first item, whether the key is in the second item, or whether the key is in the third item.\n(39:40.12) Of course, it is not surprising that every time you may get a different number of searches.\n(39:47.40) Sometimes you are lucky and just need one search.\n(39:50.08) Sometimes you are unlucky, and you search through the whole list and still cannot find the answer because the key is not inside.\n(39:56.92) In such a situation, you expect to analyze the time complexity. You want to know the best case, worst case, and average case because your time complexity can vary.",
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] human face\n[Transcript] (50:33.48) Therefore, I can conclude that I don't need to really bother about the probability because, in the end, whether it's in the list or not, both are linear functions.\n(50:45.08) So I can conclude that the time complexity of the sequential search is \u0398(N) in the average case.\n(51:01.76) I would say the average case.\n(51:03.64) I'm talking about the average case, yeah.\n(51:08.08) So have we bounded the complexity?\n(51:14.88) Yes, this one is always linear.\n(51:16.60) I already showed you it's linear.\n(51:19.04) You can go and prove it; this is definitely bounded.\n(51:23.88) It's tightly bounded because this is a linear function.\n(51:27.68) Of course.\n(51:30.32) OK, if you really want to follow the definition, yes, you're supposed to go and prove it.\n(51:35.04) Here I skip the steps.\n(51:37.80) If you want to prove it, I expect that you will go and prove it; you will get the answer.\n(51:44.52) The answer is still tightly bounded.\n(51:49.12) OK, so for the linear case, yeah, I have shown you already.\n(51:55.56) So it's not surprising that in this kind of sequential search, you need to search all the items, and all the data are not sorted in order.\n(52:11.08) You have no other choice; you have to check every single one.\n(52:14.52) If you want to search for the key in the linked list, you have to search every single one.\n(52:20.68) It's a very naive or brute force approach.\n(52:24.52) You search through each item.\n(52:27.16) Usually, this kind of algorithm should have a time complexity in O(N) because you need to check every item.\n(52:36.68) Normally, you just need to check them once.\n(52:39.72) Whether you store them as a linked list, an array, or a binary tree structure, you need to check every node once.\n(52:48.48) In that situation, it's very likely that you need to take linear time to search.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] software, computer\n[OCR] ge, OF, TER SCIE, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke, \u00b7 Best-case complexity: o (1), 1 comparison against key (the first item is the search key), \u00b7 Worst-case complexity: o (n), n comparison against key (Either the last item or no item is the search key), \u00b7 Average-case complexity:, Key is in the search array:, e represents the event that the key appears in ith position, so its probability P(e,) = 1/n, T(ej) is the no. of comparisons done, Both worst and average complexity are (n), Key is not in the search array:, n comparisons (if it is an linked list, you may need to take an extra comparison[Transcript] (03:46.44) I will discuss the time complexity of these two algorithms, and then we will talk about the hash table.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (45:18.04) I have N terms, so it should be N/2.\n(45:22.48) The first term is 0, and the last term is N - 1. This part cancels out.\n(45:34.76) It should be C1.\n(45:35.88) Not surprising.\n(45:36.80) Every time you need to take C1, then add C2/N, and this part is N/2.\n(45:44.72) The first term is 0, and the last term is N - 1.\n(45:47.56) The formula for the arithmetic series, yeah, then you can simplify it.\n(45:53.16) Simplifying it should give you C2/2 times N - 1.\n(46:01.08) In the end, you will find that this is a linear function.\n(46:06.56) That's why I said all the constants are actually not important.\n(46:08.84) In the end, this is a linear function.\n(46:11.12) So I can conclude that the average case time complexity of sequential search in the average case scenario is \u0398(N).\n(46:29.28) So it's in \u0398(N).\n(46:32.48) Although you can use intuition, you can also think about it: sometimes you need to search 1, sometimes 2, sometimes 3, and sometimes N. The average should be about N/2.\n(46:46.48) From here, you can also notice that it's about N/2.\n(46:51.92) The average is that if you search half, you will get an answer on average.\n(46:58.28) Sometimes you search less than half, and sometimes more than half.\n(47:02.04) So the average should be this.\n(47:04.32) Whether it's half or N, it's linear.\n(47:07.64) In the end, it's \u0398(N). So here I use \u0398(1), \u0398(N), and \u0398(N) because I'm talking about that particular scenario.\n(47:16.88) That particular scenario only applies when I talk about the best case.\n(47:21.04) The best case is definitely constant.\n(47:27.84) Of course, you can say O(1), like some books do.\n(47:31.76) Some online sources also tell you O(1) or O(N), but a more precise representation, since I only talk about one case, and this particular case is definitely linear, is to use \u0398(N).\n(47:52.16) You cannot say you are wrong if you set this to O(N); it's not the best representation since you know that it's definitely a linear function."
        ],
        "answer": "The best-case time complexity of sequential search in a linked list, as discussed in Lecture3_Sc1007, is constant time, represented as \u0398(1). This occurs when the search key is found in the first position of the list, requiring only one comparison. This is covered at [40:19].",
        "context_precision": 0.608333333318125,
        "context_recall": 1.0,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9653527184815781,
        "question_type": "SINGLE_DOC",
        "time_taken": 41.6112265586853,
        "question_index": 5
    },
    {
        "question": "In Lecture3_Sc1007, what is the worst-case time complexity of sequential search?",
        "ground_truth": "Linear time, Big Theta(N). (mentioned in 0:42:03)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (40:07.16) Sometimes you are lucky, which is the best case I have told you.\n(40:16.32) You just need one search, and you get the answer.\n(40:19.40) So no doubt that the time complexity of the best case is constant.\n(40:26.48) You just need to search C1, OK, then you don't even need to enter this while loop.\n(40:32.16) You will go straight to the answer.\n(40:37.16) Then you don't need to carry on.\n(40:38.64) The computation time is just C1.\n(40:43.32) You only need to run a constant operation.\n(40:50.24) The time complexity of the best case scenario in sequential search is Big Theta 1.\n(41:01.40) OK, worst case.\n(41:02.80) What is the worst case?\n(41:04.12) Now I am assuming that the search key is always in the list.\n(41:08.12) You definitely need to search the list.\n(41:09.64) So what is the worst case?\n(41:25.80) You can expect that you need to search all the data.\n(41:34.36) If you want to search all the data, in this example, you need to search all of them.\n(41:41.84) In the first case, you don't need to enter.\n(41:44.72) Then in the second case, after checking, if the key is not inside, you need to run this part for N - 1 iterations.\n(41:55.52) Then you reach the last one.\n(41:57.44) Unfortunately, if the key is at the last position, then you find it.\n(42:01.68) So that should be the worst case.\n(42:03.56) This means I need to run C1 + (N - 1) * C2.\n(42:10.44) Yeah, because in this part, you need to run N - 1 iterations.\n(42:15.60) In short, in the end, this is a linear function.\n(42:19.44) I can conclude that in the worst case, the time complexity of the worst case scenario is Big Theta N.\n(42:42.24) For this kind of algorithm, you can see you now have the picture.\n(42:45.24) The best case is constant, and the worst case is linear.\n(42:53.56) We also would like to know, on average, how much time you need to take.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] person, software, computer\n[OCR] ge, OF, TER SCIE, Dntu.edu.sg) Loke Yuan Ren (yrloke, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Algorithms, Hash Tables, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, O Find, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke\n[Transcript] (03:35.24) I will review a bit.\n(03:42.92) I believe you have learned this in the first half.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] software, computer\n[OCR] ge, OF, TER SCIE, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke, \u00b7 Best-case complexity: o (1), 1 comparison against key (the first item is the search key), \u00b7 Worst-case complexity: o (n), n comparison against key (Either the last item or no item is the search key), \u00b7 Average-case complexity:, Key is in the search array:, e represents the event that the key appears in ith position, so its probability P(e,) = 1/n, T(ej) is the no. of comparisons done, Both worst and average complexity are (n), Key is not in the search array:, n comparisons (if it is an linked list, you may need to take an extra comparison[Transcript] (03:46.44) I will discuss the time complexity of these two algorithms, and then we will talk about the hash table.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle-End\n[Visual labels] human face\n[Transcript] (48:04.20) Why not simply tell me it's \u0398(N) for a more precise representation?\n(48:09.36) So that is always where people get confused. If today I'm talking about the time complexity of sequential search, I didn't ask you for the best case, worst case, or average case.\n(48:20.76) So, yeah, sometimes constant, sometimes linear.\n(48:25.24) If I just want you to use one representation to represent the whole algorithm, the time complexity of this sequential search, then sometimes constant, sometimes linear. The best representation you can use is O(N).\n(48:43.52) But if I'm talking about a particular scenario, and this particular scenario is definitely the worst case, then I think it's better to use \u0398(N). Although it's not wrong to set O(N).\n(48:58.24) OK, so that is the time complexity of sequential search.\n(49:02.92) Not yet.\n(49:03.44) I think I only talked about when the key is in the list.\n(49:07.60) Someone asked what happens when the key is not in the list.\n(49:12.16) If the search key A is not in the list, then you need to search all the way to the end, and then you need to run one more iteration to end the search.\n(49:26.56) So the worst case in that scenario is C1 plus N times C2.\n(49:33.04) So it's still linear.\n(49:37.80) Since the probability of the search key being in the list is unknown, we don't know.\n(49:45.32) Unless you know all the possible data you can put in the linked list, it's very hard to determine the probability.\n(50:02.52) Generally, if you need to know the probability of being in the list and not being in the list, if it's in the list, the average is this; if not in the list, you always need to search through the whole list.\n(50:22.96) It takes C1 plus N times C2.\n(50:25.84) In either case, this is a linear function. You can see this is a linear function; this one is also a linear function.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (37:34.08) You can also set that FN is in Big O.\n(37:36.76) It's not wrong.\n(37:38.24) Yeah.\n(37:43.32) Then the big, if it's infinity, then you can only set FN is in Big Omega GN.\n(37:54.24) But OK.\n(37:54.88) What I would like to highlight is that if it's a constant, the best answer should be this because it gives you the most precise representation. It tells people if your answer is C, then they know that they have the same growth rate.\n(38:14.24) Of course, it's not wrong to say it's Big O or Big Omega, but when you tell people it's Big O or Big Omega, they may not be sure whether you are talking about this algorithm in this case or that case.\n(38:27.60) Your representation may not accurately convey the actual situation.\n(38:33.88) So I would not say it's wrong, but this will be a better and more precise answer if you get the limit as a constant and a positive constant number.\n(38:47.72) It's better to set it in Big Theta GN. OK, so we move on to an example.\n(38:59.32) You have learned the English.\n(39:01.04) When you do the linked list, the time complexity of the sequential search is a kind of sequential search.\n(39:11.64) So what is the time complexity when you are trying to search a linked list?\n(39:18.08) You need to search all the elements in the linked list, and you need to check each of them once.\n(39:27.44) You are just trying to check whether the key is in the first item, whether the key is in the second item, or whether the key is in the third item.\n(39:40.12) Of course, it is not surprising that every time you may get a different number of searches.\n(39:47.40) Sometimes you are lucky and just need one search.\n(39:50.08) Sometimes you are unlucky, and you search through the whole list and still cannot find the answer because the key is not inside.\n(39:56.92) In such a situation, you expect to analyze the time complexity. You want to know the best case, worst case, and average case because your time complexity can vary.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (45:18.04) I have N terms, so it should be N/2.\n(45:22.48) The first term is 0, and the last term is N - 1. This part cancels out.\n(45:34.76) It should be C1.\n(45:35.88) Not surprising.\n(45:36.80) Every time you need to take C1, then add C2/N, and this part is N/2.\n(45:44.72) The first term is 0, and the last term is N - 1.\n(45:47.56) The formula for the arithmetic series, yeah, then you can simplify it.\n(45:53.16) Simplifying it should give you C2/2 times N - 1.\n(46:01.08) In the end, you will find that this is a linear function.\n(46:06.56) That's why I said all the constants are actually not important.\n(46:08.84) In the end, this is a linear function.\n(46:11.12) So I can conclude that the average case time complexity of sequential search in the average case scenario is \u0398(N).\n(46:29.28) So it's in \u0398(N).\n(46:32.48) Although you can use intuition, you can also think about it: sometimes you need to search 1, sometimes 2, sometimes 3, and sometimes N. The average should be about N/2.\n(46:46.48) From here, you can also notice that it's about N/2.\n(46:51.92) The average is that if you search half, you will get an answer on average.\n(46:58.28) Sometimes you search less than half, and sometimes more than half.\n(47:02.04) So the average should be this.\n(47:04.32) Whether it's half or N, it's linear.\n(47:07.64) In the end, it's \u0398(N). So here I use \u0398(1), \u0398(N), and \u0398(N) because I'm talking about that particular scenario.\n(47:16.88) That particular scenario only applies when I talk about the best case.\n(47:21.04) The best case is definitely constant.\n(47:27.84) Of course, you can say O(1), like some books do.\n(47:31.76) Some online sources also tell you O(1) or O(N), but a more precise representation, since I only talk about one case, and this particular case is definitely linear, is to use \u0398(N).\n(47:52.16) You cannot say you are wrong if you set this to O(N); it's not the best representation since you know that it's definitely a linear function."
        ],
        "answer": "In Lecture3_Sc1007, the worst-case time complexity of sequential search is described as linear, specifically Big Theta N (\u0398(N)). This occurs when you need to search through all the elements in the list, which means you may have to check each item until you reach the last one. This is covered at [42:19.44].",
        "context_precision": 0.9999999999833333,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9835882315882993,
        "question_type": "SINGLE_DOC",
        "time_taken": 41.794337034225464,
        "question_index": 6
    },
    {
        "question": "In Lecture3_Sc1007, what is the average-case complexity of sequential search?",
        "ground_truth": "Linear time, Big Theta(N). (mentioned in 0:46:32)",
        "context": [
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (45:18.04) I have N terms, so it should be N/2.\n(45:22.48) The first term is 0, and the last term is N - 1. This part cancels out.\n(45:34.76) It should be C1.\n(45:35.88) Not surprising.\n(45:36.80) Every time you need to take C1, then add C2/N, and this part is N/2.\n(45:44.72) The first term is 0, and the last term is N - 1.\n(45:47.56) The formula for the arithmetic series, yeah, then you can simplify it.\n(45:53.16) Simplifying it should give you C2/2 times N - 1.\n(46:01.08) In the end, you will find that this is a linear function.\n(46:06.56) That's why I said all the constants are actually not important.\n(46:08.84) In the end, this is a linear function.\n(46:11.12) So I can conclude that the average case time complexity of sequential search in the average case scenario is \u0398(N).\n(46:29.28) So it's in \u0398(N).\n(46:32.48) Although you can use intuition, you can also think about it: sometimes you need to search 1, sometimes 2, sometimes 3, and sometimes N. The average should be about N/2.\n(46:46.48) From here, you can also notice that it's about N/2.\n(46:51.92) The average is that if you search half, you will get an answer on average.\n(46:58.28) Sometimes you search less than half, and sometimes more than half.\n(47:02.04) So the average should be this.\n(47:04.32) Whether it's half or N, it's linear.\n(47:07.64) In the end, it's \u0398(N). So here I use \u0398(1), \u0398(N), and \u0398(N) because I'm talking about that particular scenario.\n(47:16.88) That particular scenario only applies when I talk about the best case.\n(47:21.04) The best case is definitely constant.\n(47:27.84) Of course, you can say O(1), like some books do.\n(47:31.76) Some online sources also tell you O(1) or O(N), but a more precise representation, since I only talk about one case, and this particular case is definitely linear, is to use \u0398(N).\n(47:52.16) You cannot say you are wrong if you set this to O(N); it's not the best representation since you know that it's definitely a linear function.",
            "[Video title] Lecture3_Sc1007\n[Tags] End\n[Visual labels] human face\n[Transcript] (50:33.48) Therefore, I can conclude that I don't need to really bother about the probability because, in the end, whether it's in the list or not, both are linear functions.\n(50:45.08) So I can conclude that the time complexity of the sequential search is \u0398(N) in the average case.\n(51:01.76) I would say the average case.\n(51:03.64) I'm talking about the average case, yeah.\n(51:08.08) So have we bounded the complexity?\n(51:14.88) Yes, this one is always linear.\n(51:16.60) I already showed you it's linear.\n(51:19.04) You can go and prove it; this is definitely bounded.\n(51:23.88) It's tightly bounded because this is a linear function.\n(51:27.68) Of course.\n(51:30.32) OK, if you really want to follow the definition, yes, you're supposed to go and prove it.\n(51:35.04) Here I skip the steps.\n(51:37.80) If you want to prove it, I expect that you will go and prove it; you will get the answer.\n(51:44.52) The answer is still tightly bounded.\n(51:49.12) OK, so for the linear case, yeah, I have shown you already.\n(51:55.56) So it's not surprising that in this kind of sequential search, you need to search all the items, and all the data are not sorted in order.\n(52:11.08) You have no other choice; you have to check every single one.\n(52:14.52) If you want to search for the key in the linked list, you have to search every single one.\n(52:20.68) It's a very naive or brute force approach.\n(52:24.52) You search through each item.\n(52:27.16) Usually, this kind of algorithm should have a time complexity in O(N) because you need to check every item.\n(52:36.68) Normally, you just need to check them once.\n(52:39.72) Whether you store them as a linked list, an array, or a binary tree structure, you need to check every node once.\n(52:48.48) In that situation, it's very likely that you need to take linear time to search.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle-End\n[Visual labels] human face\n[Transcript] (48:04.20) Why not simply tell me it's \u0398(N) for a more precise representation?\n(48:09.36) So that is always where people get confused. If today I'm talking about the time complexity of sequential search, I didn't ask you for the best case, worst case, or average case.\n(48:20.76) So, yeah, sometimes constant, sometimes linear.\n(48:25.24) If I just want you to use one representation to represent the whole algorithm, the time complexity of this sequential search, then sometimes constant, sometimes linear. The best representation you can use is O(N).\n(48:43.52) But if I'm talking about a particular scenario, and this particular scenario is definitely the worst case, then I think it's better to use \u0398(N). Although it's not wrong to set O(N).\n(48:58.24) OK, so that is the time complexity of sequential search.\n(49:02.92) Not yet.\n(49:03.44) I think I only talked about when the key is in the list.\n(49:07.60) Someone asked what happens when the key is not in the list.\n(49:12.16) If the search key A is not in the list, then you need to search all the way to the end, and then you need to run one more iteration to end the search.\n(49:26.56) So the worst case in that scenario is C1 plus N times C2.\n(49:33.04) So it's still linear.\n(49:37.80) Since the probability of the search key being in the list is unknown, we don't know.\n(49:45.32) Unless you know all the possible data you can put in the linked list, it's very hard to determine the probability.\n(50:02.52) Generally, if you need to know the probability of being in the list and not being in the list, if it's in the list, the average is this; if not in the list, you always need to search through the whole list.\n(50:22.96) It takes C1 plus N times C2.\n(50:25.84) In either case, this is a linear function. You can see this is a linear function; this one is also a linear function.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] software, computer\n[OCR] ge, OF, TER SCIE, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke, \u00b7 Best-case complexity: o (1), 1 comparison against key (the first item is the search key), \u00b7 Worst-case complexity: o (n), n comparison against key (Either the last item or no item is the search key), \u00b7 Average-case complexity:, Key is in the search array:, e represents the event that the key appears in ith position, so its probability P(e,) = 1/n, T(ej) is the no. of comparisons done, Both worst and average complexity are (n), Key is not in the search array:, n comparisons (if it is an linked list, you may need to take an extra comparison[Transcript] (03:46.44) I will discuss the time complexity of these two algorithms, and then we will talk about the hash table.",
            "[Video title] Lecture3_Sc1007\n[Tags] Middle\n[Visual labels] human face\n[Transcript] (40:07.16) Sometimes you are lucky, which is the best case I have told you.\n(40:16.32) You just need one search, and you get the answer.\n(40:19.40) So no doubt that the time complexity of the best case is constant.\n(40:26.48) You just need to search C1, OK, then you don't even need to enter this while loop.\n(40:32.16) You will go straight to the answer.\n(40:37.16) Then you don't need to carry on.\n(40:38.64) The computation time is just C1.\n(40:43.32) You only need to run a constant operation.\n(40:50.24) The time complexity of the best case scenario in sequential search is Big Theta 1.\n(41:01.40) OK, worst case.\n(41:02.80) What is the worst case?\n(41:04.12) Now I am assuming that the search key is always in the list.\n(41:08.12) You definitely need to search the list.\n(41:09.64) So what is the worst case?\n(41:25.80) You can expect that you need to search all the data.\n(41:34.36) If you want to search all the data, in this example, you need to search all of them.\n(41:41.84) In the first case, you don't need to enter.\n(41:44.72) Then in the second case, after checking, if the key is not inside, you need to run this part for N - 1 iterations.\n(41:55.52) Then you reach the last one.\n(41:57.44) Unfortunately, if the key is at the last position, then you find it.\n(42:01.68) So that should be the worst case.\n(42:03.56) This means I need to run C1 + (N - 1) * C2.\n(42:10.44) Yeah, because in this part, you need to run N - 1 iterations.\n(42:15.60) In short, in the end, this is a linear function.\n(42:19.44) I can conclude that in the worst case, the time complexity of the worst case scenario is Big Theta N.\n(42:42.24) For this kind of algorithm, you can see you now have the picture.\n(42:45.24) The best case is constant, and the worst case is linear.\n(42:53.56) We also would like to know, on average, how much time you need to take.",
            "[Video title] Lecture3_Sc1007\n[Tags] Beginning\n[Visual labels] person, software, computer\n[OCR] ge, OF, TER SCIE, Dntu.edu.sg) Loke Yuan Ren (yrloke, Help, Search, Loke Yuan Ren (Dr) LY, LY, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View, Acrobat, X Cut, Replace, Paste, Designer, Format Painter, Create and Share, Dictate, Convert to SmartArt, Styles, Shape Effects, Adobe PDF, Font, Paragraph, Drawing, Editing, Adobe Acrobat, Accessibility: Investigate, Notes, Q Search, ENG, US, AutoSave Off, 2 Shape Outline, English (Singapore), Share, Lecture_03_HashTable - Read-Only . Saved to \\\\cerberus\\teaching v, Find, Slides, Clipboard, SC1007, Data Structures and, Algorithms, Hash Tables, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Binary Search - Worst Case Time complexity, Slide 1 of 28, Mostly cloudy, New, Reuse, Voice, Overview, \u00b7 Ex Laustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, Terminology, \u00b7 Closed Hashing, O Find, Time Complexity of Sequential Search, ntu.edu.sg) Loke Yuan Ren (yrloke\n[Transcript] (03:35.24) I will review a bit.\n(03:42.92) I believe you have learned this in the first half."
        ],
        "answer": "In Lecture3_Sc1007, the average-case time complexity of sequential search is concluded to be \u0398(N). This is because, on average, you would need to search through about half of the items in the list, which leads to a linear function representation. This is explicitly stated at the timestamp \"Covered at 46:11\" and further reinforced at \"Covered at 50:45\".",
        "context_precision": 0.99999999998,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9764091228692577,
        "question_type": "SINGLE_DOC",
        "time_taken": 42.741403579711914,
        "question_index": 7
    },
    {
        "question": "What improvement does the lecturer suggest over binary search for faster lookups in Lecture4_Sc1007?",
        "ground_truth": "Using a hashtable to achieve constant time search. (mentioned in 0:08:15)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, of, Hash Table, \u00b7 A typical space and time trade-off in algorithms, \u00b7 A hash table is a data structure that allows for efficient lookup,, insertion, and deletion of key-value pairs., usage will be increased significantly., \u00b7 To achieve search time in O(1), we can use hash tables but memory, SPINE LEVEL\n[Transcript] (07:39.72) However, you need to sort your data in a certain way to apply the divide and conquer strategy of binary search.\n(07:53.72) This can improve your search time from \u0398(N) to \u0398(log\u2082 N). Can we further improve our searching algorithm from logarithmic to constant time?\n(08:15.80) The answer is yes, it is possible.\n(08:18.68) This means using a hashtable.\n(08:23.64) At this stage, I want to improve the time by storing the data in a certain way, which may require more space, but I can improve my search time from logarithmic to constant time.\n(08:54.12) This kind of algorithm or approach is a trade-off between space and time.\n(09:00.68) You sacrifice space to achieve better time complexity.\n(09:07.16) A hashtable is a type of data structure, similar to a dictionary.\n(09:17.52) When you want to search, you don't need to check every single page when searching for a particular word in the dictionary.\n(09:31.48) You can go directly to the appropriate section based on the alphabet.\n(09:40.52) A hashtable allows you to efficiently look up, insert, and delete keys.\n(09:51.04) In all data structures, we typically perform three operations: retrieve, insert, or delete.\n(10:03.16) The same applies to a hashtable.",
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:12:05.28) you should consider rehashing.\n(01:12:07.40) You would create a bigger hash table and then map the old data to the larger hash table so that your \u03b1 drops. If it was 0.5, doubling it would drop it back to 0.25, allowing you to insert more new keys into the hash table.\n(01:12:27.56) This process is known as rehashing.\n(01:12:32.36) Overall, this kind of lookup table can allow you to search efficiently if you arrange it properly and use the appropriate hash table and algorithms.\n(01:12:44.20) You should be able to achieve constant time search.\n(01:12:49.32) Yes, that is why people, if they have frequently used data, will use a hash table to store it instead of using sequential search or binary search.\n(01:13:03.40) OK, before I end this section, one question today for the assignment: will all test cases be included?\n(01:13:14.80) Yes.\n(01:13:15.48) For my part, yes.\n(01:13:17.48) Are they hidden?\n(01:13:18.88) No.\n(01:13:19.12) When you submit, you should know that they will inform you, but I won't disclose them.\n(01:13:23.76) Don't ask me about the test cases, okay?\n(01:13:26.16) Yes, you will find that in my assignment, there are some things.\n(01:13:30.64) Yes, you need to think about it.\n(01:13:32.32) That's why I keep emphasizing the delete key.\n(01:13:34.32) You need to think carefully about one of the questions: when you delete a key, how do you search for it? When you insert a new key, how do you consider the delete key issue?\n(01:13:58.64) I think one of the things you need to learn is that when you have a problem, you need to consider all possible scenarios.\n(01:14:04.80) Have you considered all possible scenarios?\n(01:14:14.36) You also don't need to upload the answers in NTU Learn.\n(01:14:19.60) I only use the platform to evaluate your work.\n(01:14:24.32) Yes.\n(01:14:26.28) So, any other questions before I end the section?\n(01:14:32.32) Maybe later I will put another. Yes, maybe later.\n(01:14:37.96) I will spend another half an hour to go through the binary search. I'm just showing the proof for that.\n(01:14:50.24) I think you already know that the binary search is in log\u2082 N. The slide just shows you a more proper mathematical proof on binary search only.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] Data Structures and, Algorithms, ge, LOF, Dr. Loke Yuan Ren, Jump, College of Engineering, School of Computer Science and Engineering, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, analysis, Science, Depth, Depth analysi, Best, Depth analysis, of, quelle, lea, ecnease, average, menge, Le, Lecturer, yrloke@ntu.edu.s, Has, Hash Tables, yrloke@ntu.edu.sg, Overview, \u00b7 Exhaustive Algorithm: Sequential Search, \u00b7 Decrease-and-conquer Algorithm: Binary Search, \u00b7 Data Structure Approach:, \u00b7 Open Hashing, \u00b7 Closed Hashing, espace, age\n[Transcript] (00:09.56) Hi, good afternoon.\n(00:42.82) Testing, wow, 145-second delay.\n(00:54.02) So this lecture will cover the hashtable.\n(01:01.16) I will wait until 3:30 to start the lecture, and tonight I will release the assignment about this hashtable.\n(01:19.04) Today, I will cover the hashtable first.\n(01:25.56) Last Monday, we covered the time complexity of sequential search and binary search in the first half when we discussed linked lists and binary search trees.\n(01:41.32) On Monday, we discussed the time complexity of sequential search or linear search.\n(01:53.36) Today, I will first talk about the hashtable, and if time allows, I will discuss the time complexity of binary search.\n(02:26.76) Today's topic will focus on the hashtable.\n(02:32.80) I will talk about it at 3:30, and the assignment will be released tonight, which means you will have 15 days to complete it.\n(02:47.20) Today is Wednesday, so two weeks later, on Thursday at 11:59, you must submit it.\n(02:55.68) For my part, we will only evaluate your work on the platform, so you don't have to upload your answer on NTU Learn.\n(03:07.28) I will not consider anything uploaded to NTU Learn.\n(03:09.64) I will base my evaluation on what you submit on the platform.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 70, 23, 93, 14, 31, 73, 94, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, of, Time Complexity of Sequential Search, \u00b7 Average-case complexity:, \u00b7 Best-case complexity: o (1), 1 comparison against key (the first item is the search key), \u00b7 Worst-case complexity: o (n), n comparison against key (Either the last item or no item is the search key), Key is in the search array:, Key is not in the search array:, ej represents the event that the key appears in ith position, so its probability P(e;) = 1/n, T(e;) is the no. of comparisons done, n comparisons (if it is an linked list, you may need to take an extra comparison, Both worst and average complexity are (n), Terminology, . The Height of a tree: The number of edges on the longest path from, the root to a leaf, \u00b7 The Depth/Level of a node: The number of edges from the node to, the root of its tree., For a complete binary tree with height H, we have:, where n is an integer and the size of the tree, (eg. 7<n \u226415 = 8 <n<16), H \u2264log2n < H+1, If H is an integer, H+1 must be the next integer., Minimal Height = Llog2n, ei represents the event that the key appears in ith position, H \u2264 log2n < H+1, so its probability P(e\u00a1) = 1/n, 2H-1<n\u2264 2H+1 -1\n[Transcript] (06:09.88) The part about binary search was covered in the first half by Dr. Owen.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 70, 23, 93, 14, 31, 73, 99, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, of, Binary Search - Average Case Time, Complexity, \u00b7 Af(n): # of comparisons for unsuccessful search (worst case): 0(log2 n), \u00b7 As (n): # of comparisons for successful search, \u00b7 For As(n), we assume n = 2k - 1 first, 1 comparison, 2 comparisons, . We can observe that:, \u00b7 1 position requires 1 comparison, \u00b7 2 positions requires 2 comparisons, \u00b7 2t-1 positions requires t comparisons, \u00b7 n=2k-1, we have, log2(n+1), \u00b7 The time complexity is, \u00b7 Binary search does aoproximately log2 (n + 1) comparisons on, average for n entries., \u00b7 q is probability which is always \u2264 1, is very small especially when n >> 1, Hash Table, \u00b7 A typical space and time trade-off in algorithms, \u00b7 A hash table is a data structure that allows for efficient lookup,, insertion, and deletion of key-value pairs., To achieve search time in O(1), we can use hash tables but memory, usage will be increased significantly., \u00b7 Binary search does approximately log2 (n + 1) comparisons on, \u00b7 To achieve search time in O(1), we can use hash tables but memory\n[Transcript] (06:44.20) I will temporarily skip this part for today.\n(06:48.20) I will analyze the best case and worst case, whether the key is in the tree or not.\n(06:57.72) On average, the conclusion is still logarithmic.\n(07:03.80) The time complexity of binary search in the average case is \u0398(log\u2082 N).\n(07:17.76) Later, if I have time, I will talk about it.\n(07:20.92) But today, I want to focus on the hashtable.\n(07:25.36) You can search in linear time using sequential search.\n(07:31.40) You can improve it by using the divide and conquer strategy of binary search.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (50:31.64) If this particular slot is already occupied by some earlier data you inserted, then it will look for an alternative location where the address is open.\n(50:44.28) They are not fixed in that particular location; if this location is already occupied, they will try to find an alternative location or slot.\n(50:58.12) The prompt is required to search for the alternate location to insert the data.\n(51:08.24) The key is still stored in this case; the key is stored in the table itself.\n(51:16.48) If your hash table has, in this example, seven slots, that means you can only insert up to seven items.\n(51:27.80) This implies that in open addressing, your alpha is definitely less than or equal to 1 because your n / H; your N can only be equal to H in the worst case when your table is full.\n(51:48.52) So the most intuitive way is called linear probing.\n(51:53.48) When you want to think of the alternative, I will introduce three probing methods.\n(51:58.72) The first one is linear probing, which is very intuitive.\n(52:01.72) For example, I want to insert 413-1911 into this particular hash table.\n(52:09.28) I use a simple hash function, just mod H. My hash is 7.\n(52:19.32) I only have 7 slots, so I simply calculate 4 mod 7, which gives me 4.\n(52:25.44) I will insert 4 into the slot at index 4.\n(52:33.44) Then I want to insert 1313. 1313 mod 7 gives me 6, so I insert it into slot 6. 19 mod 7 gives me 5, so I insert it at 5.\n(52:53.16) The last key is 11. 11 mod 7 gives me 4, so a collision occurs at the first slot.\n(53:01.56) Linear probing is straightforward.\n(53:04.48) You simply add one.\n(53:09.32) If the first insertion is at index I, the range is from 0 to hash - 1. So the first one is +0.\n(53:18.76) Now, since a collision occurs at this location, I try to add one.\n(53:21.32) The next location will be (4 + 1) mod 7."
        ],
        "answer": "The lecturer suggests using a hash table as an improvement over binary search for faster lookups. While binary search improves search time to \u0398(log\u2082 N), a hash table can achieve constant time search, O(1), by storing data in a way that allows for efficient lookup, insertion, and deletion of key-value pairs. This approach involves a trade-off between space and time, where more memory is used to achieve better time complexity. This is covered at [08:15] and [09:00].",
        "context_precision": 0.94999999997625,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9734346103746812,
        "question_type": "SINGLE_DOC",
        "time_taken": 81.24922633171082,
        "question_index": 8
    },
    {
        "question": "In Lecture4_Sc1007, what trade-off is mentioned when using hash tables?",
        "ground_truth": "Sacrificing space to achieve faster (constant time) search. (mentioned in 0:09:00)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, of, Hash Table, \u00b7 A typical space and time trade-off in algorithms, \u00b7 A hash table is a data structure that allows for efficient lookup,, insertion, and deletion of key-value pairs., usage will be increased significantly., \u00b7 To achieve search time in O(1), we can use hash tables but memory, SPINE LEVEL\n[Transcript] (07:39.72) However, you need to sort your data in a certain way to apply the divide and conquer strategy of binary search.\n(07:53.72) This can improve your search time from \u0398(N) to \u0398(log\u2082 N). Can we further improve our searching algorithm from logarithmic to constant time?\n(08:15.80) The answer is yes, it is possible.\n(08:18.68) This means using a hashtable.\n(08:23.64) At this stage, I want to improve the time by storing the data in a certain way, which may require more space, but I can improve my search time from logarithmic to constant time.\n(08:54.12) This kind of algorithm or approach is a trade-off between space and time.\n(09:00.68) You sacrifice space to achieve better time complexity.\n(09:07.16) A hashtable is a type of data structure, similar to a dictionary.\n(09:17.52) When you want to search, you don't need to check every single page when searching for a particular word in the dictionary.\n(09:31.48) You can go directly to the appropriate section based on the alphabet.\n(09:40.52) A hashtable allows you to efficiently look up, insert, and delete keys.\n(09:51.04) In all data structures, we typically perform three operations: retrieve, insert, or delete.\n(10:03.16) The same applies to a hashtable.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table, n and its resolutions, table - Dynamic hash table, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements.\n[Transcript] (13:58.32) No matter how you design it, as I mentioned, you are trying to map data of arbitrary size to a fixed size in the hash table. The hash table is much smaller than all the possible data you can have.\n(14:19.80) In the end, when you do this mapping, you will definitely encounter situations where two or more data items are trying to map to the same location.\n(14:30.16) This situation is known as a collision.\n(14:34.96) You have two different values, but through the hash function, they map to the same slot or index.\n(14:46.44) In such a case, it is known as a collision.\n(14:49.24) Resolving such a collision is a problem we need to discuss in the context of hash tables.\n(14:57.72) In a hash table, you can insert data.\n(15:01.72) Of course, you can delete data as well.\n(15:03.04) Deleting a key may not be intuitive; you cannot simply delete it.\n(15:09.16) We will discuss that later.\n(15:11.56) As you keep inserting data into the hash table, eventually, you will encounter a situation where your hash table is full.\n(15:20.20) When your hash table is full, how are you going to resize it?\n(15:26.12) That is another issue we will discuss later.\n(15:31.72) The most intuitive way to build a hash table is like a direct address table.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, of, Some Applications of Hash Tables, . Caching: Hash tables are commonly used in caching applications to store frequently accessed, data. The hash table can be used to store key-value pairs, where the key is a unique identifier for, the data and the value is the data itself., \u00b7 Databases: Hash tables are often used in databases to provide fast lookups of data. For example,, a hash table can be used to store a table of users, with the user ID as the key and the user's, information as the value., \u00b7 Counting: Hash tables can be used to count occurrences of items in a dataset. Each item can be, stored as a key in the hash table, vrith the value representing the count of occurrences. This can, be used in applications like word frequency analysis, where the frequency of words in a document, is analyzed., \u00b7 Cryptography: Hash tables are used in cryptography to store password hashes. When a user logs, in, their password is hashed and compared to the hashed value stored in the hash table. This, allows for secure authentication without storing the actual passwords in the hash table., oke Yuan Ren (yrloke, OF\n[Transcript] (12:16.64) This improves overall performance because frequently used data can be retrieved easily, while less frequently used data may take more time to access.\n(12:32.92) Another application is cryptography.\n(12:39.52) Passwords set on computers may use a hashtable for storage.\n(12:46.12) Instead of comparing passwords directly, they compare hash values to verify the password without exposing it publicly.\n(12:55.32) Actual cryptography is more complicated, but it often involves some form of a hashtable.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, Some Applications of Hash Tables, . Caching: Hash tables are commonly used in caching applications to store frequently accessed, data. The hash table can be used to store key-value pairs, where the key is a unique identifier for, the data and the value is the data itself., \u00b7 Databases: Hash tables are often used in databases to provide fast lookups of data. For example,, a hash table can be used to store a table of users, with the user ID as the key and the user's, information as the value., \u00b7 Counting: Hash tables can be used to count occurrences of items in a dataset. Each item can be, stored as a key in the hash table, vrith the value representing the count of occurrences. This can, be used in applications like word frequency analysis, where the frequency of words in a document, is analyzed., \u00b7 Cryptography: Hash tables are used in cryptography to store password hashes. When a user logs, in, their password is hashed and compared to the hashed value stored in the hash table. This, allows for secure authentication without storing the actual passwords in the hash table., OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table\n[Transcript] (13:20.44) What is hashing?\n(13:21.64) Hashing is the process of using a hash table to map data of arbitrary size to a fixed size.\n(13:29.64) You have a lot of data, and eventually, you want to map it to a fixed size array or table.\n(13:39.44) There is a mapping, which is a hash function.\n(13:43.44) To design a hash table, there are a few things we need to consider. Of course, we need to know how the data is distributed so that we can decide on the appropriate hash function.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R\n[Transcript] (18:25.08) The purpose of having a hash function is to reduce your key space to a reasonable size.\n(19:01.12) However, the hash function cannot be too complicated to evaluate.\n(19:06.40) It still needs to be computable in constant time so that my search time remains at O(1) on average.\n(19:18.04) That is the hash function we look for.\n(19:22.12) The array will become known as a hash table, and the slots will be referred to as such.\n(19:37.76) However, this implies that there is a chance that multiple keys may map to the same slot or return the same hash value, requiring them to be placed in the same hash slot.\n(19:55.52) This is known as a collision.\n(19:58.92) When a collision occurs, we need to resolve it.\n(20:03.60) What should we do?\n(20:17.84) When you create a table, the number of slots will be fixed and will be less than the number of data items you have.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., 3., 1., ge, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc., 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts\n[Transcript] (23:42.28) It should not depend on the problem size.\n(23:45.76) This means that whatever key or value you give to the hash function, it should complete in constant time.\n(24:00.56) Additionally, you need to minimize collisions.\n(24:04.32) You should try to avoid having two different keys map to the same index.\n(24:18.40) However, you cannot completely avoid collisions because the number of data items is usually much greater than the size of your hash table.\n(24:30.92) Eventually, some keys will return the same value through your hash function, resulting in a collision.\n(24:44.16) We can only try to minimize it, but we cannot avoid it entirely.\n(24:52.32) Therefore, there are a few examples of hash functions.\n(24:59.96) Typical ones include modular, arithmetic folding, mid-square, and multiplicative congruential methods, among others.\n(25:09.20) There are many methods for you to choose from when designing your hash function.\n(25:16.76) The typical one is the first one, modular arithmetic."
        ],
        "answer": "In Lecture4_Sc1007, the trade-off mentioned when using hash tables is between space and time. Specifically, to achieve better time complexity for operations like searching, insertion, and deletion (aiming for O(1) time), you may need to sacrifice memory space. This means that while you can improve the efficiency of data retrieval, it may require using more memory than other data structures. This trade-off is highlighted when the speaker states, \"You sacrifice space to achieve better time complexity\" and \"This kind of algorithm or approach is a trade-off between space and time\" at [08:54.12] and [09:00.68].",
        "context_precision": 0.81666666664625,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9874569978586855,
        "question_type": "SINGLE_DOC",
        "time_taken": 42.06273651123047,
        "question_index": 9
    },
    {
        "question": "In Lecture4_Sc1007, what is hashing defined as?",
        "ground_truth": "Mapping data of arbitrary size to a fixed size array using a hash function. (mentioned in 0:13:20)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table, n and its resolutions, table - Dynamic hash table, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements.\n[Transcript] (13:58.32) No matter how you design it, as I mentioned, you are trying to map data of arbitrary size to a fixed size in the hash table. The hash table is much smaller than all the possible data you can have.\n(14:19.80) In the end, when you do this mapping, you will definitely encounter situations where two or more data items are trying to map to the same location.\n(14:30.16) This situation is known as a collision.\n(14:34.96) You have two different values, but through the hash function, they map to the same slot or index.\n(14:46.44) In such a case, it is known as a collision.\n(14:49.24) Resolving such a collision is a problem we need to discuss in the context of hash tables.\n(14:57.72) In a hash table, you can insert data.\n(15:01.72) Of course, you can delete data as well.\n(15:03.04) Deleting a key may not be intuitive; you cannot simply delete it.\n(15:09.16) We will discuss that later.\n(15:11.56) As you keep inserting data into the hash table, eventually, you will encounter a situation where your hash table is full.\n(15:20.20) When your hash table is full, how are you going to resize it?\n(15:26.12) That is another issue we will discuss later.\n(15:31.72) The most intuitive way to build a hash table is like a direct address table.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, Some Applications of Hash Tables, . Caching: Hash tables are commonly used in caching applications to store frequently accessed, data. The hash table can be used to store key-value pairs, where the key is a unique identifier for, the data and the value is the data itself., \u00b7 Databases: Hash tables are often used in databases to provide fast lookups of data. For example,, a hash table can be used to store a table of users, with the user ID as the key and the user's, information as the value., \u00b7 Counting: Hash tables can be used to count occurrences of items in a dataset. Each item can be, stored as a key in the hash table, vrith the value representing the count of occurrences. This can, be used in applications like word frequency analysis, where the frequency of words in a document, is analyzed., \u00b7 Cryptography: Hash tables are used in cryptography to store password hashes. When a user logs, in, their password is hashed and compared to the hashed value stored in the hash table. This, allows for secure authentication without storing the actual passwords in the hash table., OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table\n[Transcript] (13:20.44) What is hashing?\n(13:21.64) Hashing is the process of using a hash table to map data of arbitrary size to a fixed size.\n(13:29.64) You have a lot of data, and eventually, you want to map it to a fixed size array or table.\n(13:39.44) There is a mapping, which is a hash function.\n(13:43.44) To design a hash table, there are a few things we need to consider. Of course, we need to know how the data is distributed so that we can decide on the appropriate hash function.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R\n[Transcript] (18:25.08) The purpose of having a hash function is to reduce your key space to a reasonable size.\n(19:01.12) However, the hash function cannot be too complicated to evaluate.\n(19:06.40) It still needs to be computable in constant time so that my search time remains at O(1) on average.\n(19:18.04) That is the hash function we look for.\n(19:22.12) The array will become known as a hash table, and the slots will be referred to as such.\n(19:37.76) However, this implies that there is a chance that multiple keys may map to the same slot or return the same hash value, requiring them to be placed in the same hash slot.\n(19:55.52) This is known as a collision.\n(19:58.92) When a collision occurs, we need to resolve it.\n(20:03.60) What should we do?\n(20:17.84) When you create a table, the number of slots will be fixed and will be less than the number of data items you have.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., ge, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc.\n[Transcript] (20:26.56) You have N data items to store in H slots.\n(20:30.72) There is a ratio known as the load factor.\n(20:35.52) The load factor helps evaluate how many slots are in use.\n(20:41.44) If all the slots are in use, the load factor will be equal to 1, meaning N equals H, where N is the number of data items and H is the number of slots.\n(20:59.88) If each slot contains only one item, when your slots are full, you have H slots filled with data.\n(21:09.96) At that moment, your load factor is equal to 1.\n(21:17.48) If 50% of the slots are in use, your load factor will be equal to 0.5.\n(21:24.64) This load factor allows you to judge the load of your hash table.\n(21:31.76) These are just some terms we will use.\n(21:34.72) How do we design the hash function?\n(21:36.68) There are a few criteria you need to consider.\n(21:39.32) You can think of many ways to design it.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements., Universe of Keys, U, Actual Keys, K, 6 data, 8 data, Figure 4.1: Direct Address Table, 3 data, 9 dat, 5 data, 5 dat, 3 dat, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -\n[Transcript] (17:17.56) By doing that, you will find that 80-90% of your storage will not be in use because your actual keys may be much smaller than all the possible keys you can have.\n(17:33.96) This is a very naive approach: a direct address table.\n(17:45.68) I am trying to create an actual key through a function to make it a smaller array to store it.\n(17:57.12) Then I can save some space, but the effect is still the same.\n(18:02.04) If my hash function is very simple to calculate and can be done in constant time, I just create a function to get a new index, then store it.\n(18:16.64) I can still achieve search time in constant time.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning-Middle\n[OCR] 14, 13, 11, 12, 10, 21, 24, 19, 15, 16, 29, 27, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . The key is squared and the middle part of the result is used as the hash, address, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 -> H(k) = 406, \u00b7 Pseudo-random number generator, \u00b7 H(k) = (a x k) mod h, 10 k -, 1 14 27, 1 2 3, 21 3, 14 15, 8 2, 13 2, 3 16, . Shift folding: Divide the key into a few parts and added up these parts, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 > H(k) = 406, 3 16 29 11 24, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing\n[Transcript] (31:02.96) In the lecture, we will discuss all the examples.\n(31:06.92) Usually, we will use the first method, modular arithmetic.\n(31:10.36) We simply mod the hash table size to map the key to a particular index.\n(31:18.20) This is the choice of the hash function.\n(31:23.28) As I mentioned, there are collisions.\n(31:26.60) No matter how you do it, there is always a chance that two different keys will map to the same slot through your hash function, which is called a collision.\n(31:37.84) But how do you resolve the collision?"
        ],
        "answer": "In Lecture4_Sc1007, hashing is defined as the process of using a hash function to map data of arbitrary size to fixed-size values or keys. This involves creating a mapping from a large set of possible data to a smaller, fixed-size array or table, known as a hash table. The goal is to ensure that each key is mapped to a unique index (hash value/code/address) while maintaining an average search time of O(1) for efficiency. This definition is covered at [13:21].",
        "context_precision": 0.9999999999833333,
        "context_recall": 1.0,
        "faithfulness_result": 0.8571428571428571,
        "response_relevancy": 0.9767529639335906,
        "question_type": "SINGLE_DOC",
        "time_taken": 51.75357270240784,
        "question_index": 10
    },
    {
        "question": "What problem arises when two values map to the same slot in a hash table discussed in Lecture4_Sc1007?",
        "ground_truth": "A collision. (mentioned in 0:14:30)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R\n[Transcript] (18:25.08) The purpose of having a hash function is to reduce your key space to a reasonable size.\n(19:01.12) However, the hash function cannot be too complicated to evaluate.\n(19:06.40) It still needs to be computable in constant time so that my search time remains at O(1) on average.\n(19:18.04) That is the hash function we look for.\n(19:22.12) The array will become known as a hash table, and the slots will be referred to as such.\n(19:37.76) However, this implies that there is a chance that multiple keys may map to the same slot or return the same hash value, requiring them to be placed in the same hash slot.\n(19:55.52) This is known as a collision.\n(19:58.92) When a collision occurs, we need to resolve it.\n(20:03.60) What should we do?\n(20:17.84) When you create a table, the number of slots will be fixed and will be less than the number of data items you have.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., ge, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc.\n[Transcript] (20:26.56) You have N data items to store in H slots.\n(20:30.72) There is a ratio known as the load factor.\n(20:35.52) The load factor helps evaluate how many slots are in use.\n(20:41.44) If all the slots are in use, the load factor will be equal to 1, meaning N equals H, where N is the number of data items and H is the number of slots.\n(20:59.88) If each slot contains only one item, when your slots are full, you have H slots filled with data.\n(21:09.96) At that moment, your load factor is equal to 1.\n(21:17.48) If 50% of the slots are in use, your load factor will be equal to 0.5.\n(21:24.64) This load factor allows you to judge the load of your hash table.\n(21:31.76) These are just some terms we will use.\n(21:34.72) How do we design the hash function?\n(21:36.68) There are a few criteria you need to consider.\n(21:39.32) You can think of many ways to design it.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements., Universe of Keys, U, Actual Keys, K, 6 data, 8 data, Figure 4.1: Direct Address Table, 3 data, 9 dat, 5 data, 5 dat, 3 dat, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -\n[Transcript] (17:17.56) By doing that, you will find that 80-90% of your storage will not be in use because your actual keys may be much smaller than all the possible keys you can have.\n(17:33.96) This is a very naive approach: a direct address table.\n(17:45.68) I am trying to create an actual key through a function to make it a smaller array to store it.\n(17:57.12) Then I can save some space, but the effect is still the same.\n(18:02.04) If my hash function is very simple to calculate and can be done in constant time, I just create a function to get a new index, then store it.\n(18:16.64) I can still achieve search time in constant time.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table, n and its resolutions, table - Dynamic hash table, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements.\n[Transcript] (13:58.32) No matter how you design it, as I mentioned, you are trying to map data of arbitrary size to a fixed size in the hash table. The hash table is much smaller than all the possible data you can have.\n(14:19.80) In the end, when you do this mapping, you will definitely encounter situations where two or more data items are trying to map to the same location.\n(14:30.16) This situation is known as a collision.\n(14:34.96) You have two different values, but through the hash function, they map to the same slot or index.\n(14:46.44) In such a case, it is known as a collision.\n(14:49.24) Resolving such a collision is a problem we need to discuss in the context of hash tables.\n(14:57.72) In a hash table, you can insert data.\n(15:01.72) Of course, you can delete data as well.\n(15:03.04) Deleting a key may not be intuitive; you cannot simply delete it.\n(15:09.16) We will discuss that later.\n(15:11.56) As you keep inserting data into the hash table, eventually, you will encounter a situation where your hash table is full.\n(15:20.20) When your hash table is full, how are you going to resize it?\n(15:26.12) That is another issue we will discuss later.\n(15:31.72) The most intuitive way to build a hash table is like a direct address table.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (53:28.84) If a collision occurs again, I simply add 1, then 2, then 3.\n(53:38.68) You will go to the next location, which is 6, but there is still a collision.\n(53:42.60) Then you go to (6 + 3), which gives you 0.\n(53:46.92) This location is empty, so you insert it there.\n(53:49.60) This is linear probing.\n(53:51.76) However, you will soon notice that in this example, if you keep finding the next location by adding 1, you will eventually reach H - 1.\n(54:10.72) If H - 1 is reached and you still cannot find any empty slot, it means this table is already full.\n(54:21.76) It is pointless to continue, as you will just repeat the procedure.\n(54:25.48) Linear probing can lead to problems because if your key hashes to the same location, you will keep moving to adjacent locations.\n(54:47.76) If this slot is occupied, you will move to the next slot, and so on.\n(55:02.04) This will form a cluster, making it take a lot of time to search because all your keys will be clustered together.\n(55:15.28) This is not a very good idea.\n(55:23.64) Therefore, someone came up with a better solution, which is quadratic probing. Instead of simply adding one, they use a quadratic function.\n(55:32.88) This way, they do not always look for the adjacent slot.\n(55:41.20) For example, if I let my hash function H equal to 2^3, I make it as 8 slots, then C1 and C2 are set to half.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., 3., 1., ge, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc., 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts\n[Transcript] (23:42.28) It should not depend on the problem size.\n(23:45.76) This means that whatever key or value you give to the hash function, it should complete in constant time.\n(24:00.56) Additionally, you need to minimize collisions.\n(24:04.32) You should try to avoid having two different keys map to the same index.\n(24:18.40) However, you cannot completely avoid collisions because the number of data items is usually much greater than the size of your hash table.\n(24:30.92) Eventually, some keys will return the same value through your hash function, resulting in a collision.\n(24:44.16) We can only try to minimize it, but we cannot avoid it entirely.\n(24:52.32) Therefore, there are a few examples of hash functions.\n(24:59.96) Typical ones include modular, arithmetic folding, mid-square, and multiplicative congruential methods, among others.\n(25:09.20) There are many methods for you to choose from when designing your hash function.\n(25:16.76) The typical one is the first one, modular arithmetic."
        ],
        "answer": "The problem that arises when two values map to the same slot in a hash table is known as a \"collision.\" When a collision occurs, it means that two different values have been assigned to the same index or slot in the hash table, which can complicate data retrieval. The lecture discusses that resolving such collisions is a significant issue that needs to be addressed in the context of hash tables. This is covered at [14:19].",
        "context_precision": 0.9999999999833333,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9695367379711306,
        "question_type": "SINGLE_DOC",
        "time_taken": 50.11758327484131,
        "question_index": 11
    },
    {
        "question": "In Lecture4_Sc1007, why is it recommended to make the table size a prime number?",
        "ground_truth": "To avoid clustering when using modular arithmetic. (mentioned in 0:27:13)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Middle-End\n[Transcript] (01:01:04.24) Then 13 will map to 6, and 19 will map to 5.\n(01:01:11.44) For 11, a collision occurs. 11 mod 7 gives me 3, so I add 1, and I need to move 3 steps to find the next available slot.\n(01:01:34.76) This is the next location.\n(01:01:53.52) This is double hashing.\n(01:01:55.76) You will find that every key will get a different kind of probe to look for alternate slots.\n(01:02:02.72) There will not be a fixed position, making it less likely to form clusters.\n(01:02:15.56) Your search time will not become linear.\n(01:02:22.20) This is why people prefer double hashing.\n(01:02:27.72) One thing to note is that you cannot arbitrarily select this hash function.\n(01:02:35.92) Think about why I need to add 1.\n(01:02:41.08) If I do not add 1, there is a chance that mod 3 will give you 0, causing you to stay in the same location.\n(01:03:02.44) You need to ensure that this hash function does not give you 0, as you will remain in the same location.\n(01:03:18.68) You also need to choose a prime number to ensure that the number you use is not a factor of the slot size.\n(01:03:29.44) Otherwise, you will not be able to visit every slot in a particular sequence, and you need to ensure that you can visit all possible slots.\n(01:03:49.60) So that's why they need to choose the prime number.\n(01:03:53.24) Yes.\n(01:03:53.44) The reason I need to choose the prime number earlier is due to the real data.",
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:04:00.60) I need to choose the prime number here because I need to ensure my probe can visit every alternate slot or the possible alternate slot.\n(01:04:13.52) The time complexity for linear probing and double hashing is just for your information.\n(01:04:20.04) I will not prove it.\n(01:04:22.80) You also don't have to know how to prove it, but if you want to know how to prove it, you can check this particular book by this author.\n(01:04:30.96) This is a very well-known author.\n(01:04:35.16) Yes, he's still around.\n(01:04:36.64) He developed this concept in 1973.\n(01:04:40.36) This is quite a well-known book in computer science.\n(01:04:43.20) You can go and read it.\n(01:04:44.32) The key point I want you to note is that all this time complexity is a function of 1 / (1 - \u03b1).\n(01:04:54.56) What does it mean when your \u03b1 is close to 1?\n(01:04:57.88) For open addressing, your \u03b1 must be less than or equal to 1, right?\n(01:05:07.16) So when you are close to 1, your time complexity will grow very fast and become very high.\n(01:05:14.24) Yes, when your \u03b1 is more than 1.\n(01:05:16.84) So we need to keep \u03b1 as low as possible.\n(01:05:21.76) Usually, we try not to exceed 50% of the slots.\n(01:05:26.32) Yes.\n(01:05:26.80) This way, your search time can remain manageable.\n(01:05:33.56) The next topic is the delete key.\n(01:05:37.72) Deleting a key is not that simple because for open addressing, while for closed addressing, it is quite easy.\n(01:05:44.32) You can simply delete it without any problem, just like deleting a key from a linked list. However, deleting a key in open addressing requires caution because if you delete the key without marking it,\n(01:05:52.32) when you search for it, you may miss it because the slot appears empty, but there could be another key in the subsequent slots.\n(01:06:11.80) If you leave it as an empty slot, you won't be able to search for the subsequent key behind it.\n(01:06:26.40) So instead of actually deleting the key, we don't do that.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts, convenient way\n[Transcript] (27:13.80) To avoid this, we usually choose the size of the table to be a prime number.\n(27:48.36) This prime number should not be too close to any power of 2 to avoid clustering.\n(27:58.76) All the data clusters in certain locations, leading to collisions.\n(28:03.56) Even if you design a very nice hash function, if your data distribution is not random, all the data will try to map to the same place.\n(28:15.32) This is something you need to take note of when using modular arithmetic with real data.\n(28:22.60) Other types of hash functions you can choose include partitioning your key into several parts, performing some calculations, and then combining them.\n(28:35.04) Some people may shift, divide the key, and then add it back.\n(28:40.52) Typically, if your input key is a string, such as a word or a sentence, you want to store that raw data.\n(28:50.80) You may consider every alphabet in ASCII.\n(28:55.12) Then you can break it into several parts, sum them up, and come up with a value.\n(29:04.96) This particular word or sentence will be stored in a specific hash slot.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (58:25.32) If two keys have the same initial hash, they will definitely follow the same sequence (1, 3, 6, 2, 7, 5, 4).\n(58:34.12) This means your search time will increase because you will always move in the same pattern.\n(58:47.32) Even with different keys, you will still have to go through (1, 3, 6, 2, 7, 5, 4).\n(58:54.84) For example, if I insert 4, I will insert it into this location mod 8.\n(59:03.88) Then 13 mod 8 gives me 5, so I will insert it here.\n(59:11.24) For 19, mod 8 gives me 3, and for 11, there is a collision. 11 mod 8 gives me 3, so I add 1, and then I will be here.\n(59:38.04) But you will soon realize that in this example, it may not be obvious, but if you try larger data, you will find that you always move in (1, 3, 6, 2, 7, 5, 4).\n(59:49.40) Whatever key hashes to the same initial location will have to go through (1, 3, 6, 2, 7, 5, 4).\n(59:57.64) This will increase your search time.\n(01:00:03.64) Since this depends on my key, there is double hashing.\n(01:00:12.64) In double hashing, the probing depends on another hash function, which decides the number of steps I need to move.\n(01:00:22.24) This makes the probing somewhat random, not exactly random, but more varied.\n(01:00:32.20) You need to ensure that your hash table size must be a prime number.\n(01:00:41.20) In this example, if I try to insert 4, 13, and 19, if 4 mod 7 gives me 0, it maps to 0 initially with no collision.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, Hash Functions, 2. Folding, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts, unknown lower p-bit patterns > h avoid to, powers of 2\n[Transcript] (25:21.52) You simply take the key and apply the modulus operation with the size of the array.\n(25:30.96) For example, if my size is 10, I will take the key modulo 10.\n(25:35.76) Whatever integer key value you give me, I will simply take it modulo 10 to determine the corresponding index or hash value.\n(25:49.44) You then store the data in that location.\n(25:53.12) Usually, with these kinds of hash functions, we avoid using powers of ten, such as 10, 100, 1000, and so on.\n(26:12.64) We do it based on the statistics. A lot of daily life data is in powers of them.\n(26:25.88) If you do this, you will find that a lot of similar data is always trying to map to the same slot.\n(26:36.36) This is something we need to avoid. To avoid it, try not to choose your hash table size in powers of 10.\n(26:46.88) Another option is powers of 2 because a lot of data in the digital world today, since everything is digitized, follows a pattern of powers of 2.\n(26:59.16) If your hash table size is in terms of 2 raised to a power, you will find that your data also follows this pattern, and very likely, those data will always map to the same slot.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning-Middle\n[OCR] 14, 13, 11, 12, 10, 21, 24, 19, 15, 16, 29, 27, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . The key is squared and the middle part of the result is used as the hash, address, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 -> H(k) = 406, \u00b7 Pseudo-random number generator, \u00b7 H(k) = (a x k) mod h, 10 k -, 1 14 27, 1 2 3, 21 3, 14 15, 8 2, 13 2, 3 16, . Shift folding: Divide the key into a few parts and added up these parts, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 > H(k) = 406, 3 16 29 11 24, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing\n[Transcript] (31:02.96) In the lecture, we will discuss all the examples.\n(31:06.92) Usually, we will use the first method, modular arithmetic.\n(31:10.36) We simply mod the hash table size to map the key to a particular index.\n(31:18.20) This is the choice of the hash function.\n(31:23.28) As I mentioned, there are collisions.\n(31:26.60) No matter how you do it, there is always a chance that two different keys will map to the same slot through your hash function, which is called a collision.\n(31:37.84) But how do you resolve the collision?"
        ],
        "answer": "In Lecture4_Sc1007, it is recommended to make the hash table size a prime number to ensure that the probing can visit every alternate slot or possible alternate slot effectively. This choice helps to avoid clustering and ensures that the hash function can distribute keys more uniformly across the table. If the table size is not a prime number, it may lead to situations where certain slots cannot be accessed, which can increase the likelihood of collisions and degrade search performance. This is emphasized at the timestamps: \"Covered at 01:03:49.60\" and \"Covered at 01:04:00.60\".",
        "context_precision": 0.8099999999838,
        "context_recall": 1.0,
        "faithfulness_result": 0.875,
        "response_relevancy": 0.9611870515613701,
        "question_type": "SINGLE_DOC",
        "time_taken": 54.61216688156128,
        "question_index": 12
    },
    {
        "question": "Mentioned in Lecture4_Sc1007, what collision resolution technique stores multiple keys in a linked list at the same slot?",
        "ground_truth": "Closed addressing (separate chaining). (mentioned in 0:31:01)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:09:15.28) Yes.\n(01:09:15.56) It's very time-consuming.\n(01:09:19.64) Yes.\n(01:09:20.08) So we try not to do it too frequently.\n(01:09:22.96) When we want to increase the size, we should double the size of the table.\n(01:09:29.20) OK, I think today I have overrun a bit.\n(01:09:34.28) Sorry about that.\n(01:09:35.32) Today, I just covered the hash table.\n(01:09:39.56) You need to know that to implement a hash table, we want to improve the search time from linear to logarithmic to constant time.\n(01:09:49.72) However, to achieve this, you need to sacrifice space.\n(01:09:53.24) You need to use more space and create a very large hash table to store the data.\n(01:09:59.60) There are a few methods: open addressing and closed addressing.\n(01:10:04.32) We introduced closed addressing, which is simple.\n(01:10:06.92) Each slot is attached to a linked list.\n(01:10:12.12) When there is a collision, meaning two keys map to the same slot, you just store them in the linked list.\n(01:10:20.28) Such closed hashing will give you a logarithmic factor.\n(01:10:26.96) It can be more than one.\n(01:10:29.24) Then there is open addressing.\n(01:10:31.64) Open addressing means if a slot is already occupied and a collision occurs, you simply use linear probing, quadratic probing, or double hashing to find an alternate location.\n(01:10:47.48) If the alternate location is occupied, you find another alternate location.\n(01:10:52.56) You will keep searching every possible slot in the hash table until you reach an empty slot.\n(01:11:00.16) Because of this, you need to be careful when deleting keys under open addressing. When you delete a key, you cannot simply leave it empty because when you search, you will miss the key.\n(01:11:17.08) So when we delete a key, we don't actually delete it.\n(01:11:31.92) However, when you search, you cannot stop at this deleted key.\n(01:11:36.40) You have to continue searching because there may still be keys behind it.\n(01:11:45.20) Yes, the last topic I will discuss today is rehashing.\n(01:11:50.36) When your hash table grows to a certain point, particularly in open addressing, when you reach a certain threshold, let's say when \u03b1 is more than 0.5,",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[OCR] 84, 39, 92, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing, Closed Address Hashing: Separate Chaining, \u00b7 Keys are not stored in the table itself, \u00b7 All the keys with the same hash address are store in a separate list, . During searching, the searched element with hash address i is compared with elements in linked, list H[i] sequentially, \u00b7 In closed address hashing, there will be a number of elements in each linked list on average., h(84) = h(39) = h(92) = i, Lo, uan Ren, e Yuan Ren (yrloke@ntu.edu.sg) Lo\n[Transcript] (31:40.56) Here, there are two options. There are more, but we will only introduce two. One is closed addressing hashing, also known as separate chaining.\n(31:51.80) The idea is quite simple. Since they map to the same location, why not store all the keys that map to the same location in a linked list and store the head pointer of the linked list in the hash table?\n(32:16.48) When you search for a key, your hash function brings you to this location. At this location, there is a head pointer that leads you to the linked list.\n(32:30.08) When you reach the linked list, you search along it to check whether the key is present.\n(32:39.88) This method is called closed addressing hashing.\n(32:45.28) This means your address is fixed. If you give me the key, I always use the same hash function to go to this address to search for the key.\n(33:01.04) In short, the key is not stored in the table itself; we only store the linked list.\n(33:13.04) Each hash slot comes with a linked list at the back.\n(33:19.80) All keys with the same hash address are stored in a separate list.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (53:28.84) If a collision occurs again, I simply add 1, then 2, then 3.\n(53:38.68) You will go to the next location, which is 6, but there is still a collision.\n(53:42.60) Then you go to (6 + 3), which gives you 0.\n(53:46.92) This location is empty, so you insert it there.\n(53:49.60) This is linear probing.\n(53:51.76) However, you will soon notice that in this example, if you keep finding the next location by adding 1, you will eventually reach H - 1.\n(54:10.72) If H - 1 is reached and you still cannot find any empty slot, it means this table is already full.\n(54:21.76) It is pointless to continue, as you will just repeat the procedure.\n(54:25.48) Linear probing can lead to problems because if your key hashes to the same location, you will keep moving to adjacent locations.\n(54:47.76) If this slot is occupied, you will move to the next slot, and so on.\n(55:02.04) This will form a cluster, making it take a lot of time to search because all your keys will be clustered together.\n(55:15.28) This is not a very good idea.\n(55:23.64) Therefore, someone came up with a better solution, which is quadratic probing. Instead of simply adding one, they use a quadratic function.\n(55:32.88) This way, they do not always look for the adjacent slot.\n(55:41.20) For example, if I let my hash function H equal to 2^3, I make it as 8 slots, then C1 and C2 are set to half.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hashing, \u00b7 Hashing is the process of using a hash function to map data of, arbitrary size to fixed-size values or keys., \u00b7 To design a hash table for data retrieval, we need to consider, \u00b7 Collision and its resolutions, \u00b7 Delete a key from a hash table, \u00b7 Resizing hash table - Dynamic hash table, n and its resolutions, table - Dynamic hash table, Direct-Address Table, . Assume that the keys of elements K drawn from the universe of, possible keys U, \u00b7 No two elements have the same key, \u00b7 Search time is O(1) but ..., . The array size is enormous, \u00b7 if the keys are integers between 1 and 1000, the array used to implement the direct-, address table must have 1000 elements.\n[Transcript] (13:58.32) No matter how you design it, as I mentioned, you are trying to map data of arbitrary size to a fixed size in the hash table. The hash table is much smaller than all the possible data you can have.\n(14:19.80) In the end, when you do this mapping, you will definitely encounter situations where two or more data items are trying to map to the same location.\n(14:30.16) This situation is known as a collision.\n(14:34.96) You have two different values, but through the hash function, they map to the same slot or index.\n(14:46.44) In such a case, it is known as a collision.\n(14:49.24) Resolving such a collision is a problem we need to discuss in the context of hash tables.\n(14:57.72) In a hash table, you can insert data.\n(15:01.72) Of course, you can delete data as well.\n(15:03.04) Deleting a key may not be intuitive; you cannot simply delete it.\n(15:09.16) We will discuss that later.\n(15:11.56) As you keep inserting data into the hash table, eventually, you will encounter a situation where your hash table is full.\n(15:20.20) When your hash table is full, how are you going to resize it?\n(15:26.12) That is another issue we will discuss later.\n(15:31.72) The most intuitive way to build a hash table is like a direct address table.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., ge, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc.\n[Transcript] (20:26.56) You have N data items to store in H slots.\n(20:30.72) There is a ratio known as the load factor.\n(20:35.52) The load factor helps evaluate how many slots are in use.\n(20:41.44) If all the slots are in use, the load factor will be equal to 1, meaning N equals H, where N is the number of data items and H is the number of slots.\n(20:59.88) If each slot contains only one item, when your slots are full, you have H slots filled with data.\n(21:09.96) At that moment, your load factor is equal to 1.\n(21:17.48) If 50% of the slots are in use, your load factor will be equal to 0.5.\n(21:24.64) This load factor allows you to judge the load of your hash table.\n(21:31.76) These are just some terms we will use.\n(21:34.72) How do we design the hash function?\n(21:36.68) There are a few criteria you need to consider.\n(21:39.32) You can think of many ways to design it.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, What is hashing?, \u00b7 To reduce the key space to a reasonable size, \u00b7 Each key is mapped to a unique index (hash value/code/address), \u00b7 Search time remains O(1) on the average, hash function: {all possible keys} -> {0, 1, 2, ..., h-1}, . The array is called a hash table, \u00b7 Each entry in the hash table is called a hash slot, . When multiple keys are mapped to the same hash value, a collision occurs, . If there are n records stored in a hash table with h slots, its load factor is a = -, 1 (yrloke@ntu.edu.sg) Loke Yuan R\n[Transcript] (18:25.08) The purpose of having a hash function is to reduce your key space to a reasonable size.\n(19:01.12) However, the hash function cannot be too complicated to evaluate.\n(19:06.40) It still needs to be computable in constant time so that my search time remains at O(1) on average.\n(19:18.04) That is the hash function we look for.\n(19:22.12) The array will become known as a hash table, and the slots will be referred to as such.\n(19:37.76) However, this implies that there is a chance that multiple keys may map to the same slot or return the same hash value, requiring them to be placed in the same hash slot.\n(19:55.52) This is known as a collision.\n(19:58.92) When a collision occurs, we need to resolve it.\n(20:03.60) What should we do?\n(20:17.84) When you create a table, the number of slots will be fixed and will be less than the number of data items you have."
        ],
        "answer": "The collision resolution technique that stores multiple keys in a linked list at the same slot is known as **closed addressing hashing**, also referred to as **separate chaining**. In this method, when two keys map to the same location, they are stored in a linked list associated with that slot. This allows for efficient handling of collisions by maintaining a list of all keys that hash to the same index. \n\nThis concept is covered at **(31:40.56)** in the lecture.",
        "context_precision": 0.81666666664625,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9580307651964673,
        "question_type": "SINGLE_DOC",
        "time_taken": 60.41811537742615,
        "question_index": 13
    },
    {
        "question": "In Lecture4_Sc1007, what is linear probing in open addressing?",
        "ground_truth": "Checking the next slot sequentially when a collision occurs. (mentioned in 0:52:01)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (50:31.64) If this particular slot is already occupied by some earlier data you inserted, then it will look for an alternative location where the address is open.\n(50:44.28) They are not fixed in that particular location; if this location is already occupied, they will try to find an alternative location or slot.\n(50:58.12) The prompt is required to search for the alternate location to insert the data.\n(51:08.24) The key is still stored in this case; the key is stored in the table itself.\n(51:16.48) If your hash table has, in this example, seven slots, that means you can only insert up to seven items.\n(51:27.80) This implies that in open addressing, your alpha is definitely less than or equal to 1 because your n / H; your N can only be equal to H in the worst case when your table is full.\n(51:48.52) So the most intuitive way is called linear probing.\n(51:53.48) When you want to think of the alternative, I will introduce three probing methods.\n(51:58.72) The first one is linear probing, which is very intuitive.\n(52:01.72) For example, I want to insert 413-1911 into this particular hash table.\n(52:09.28) I use a simple hash function, just mod H. My hash is 7.\n(52:19.32) I only have 7 slots, so I simply calculate 4 mod 7, which gives me 4.\n(52:25.44) I will insert 4 into the slot at index 4.\n(52:33.44) Then I want to insert 1313. 1313 mod 7 gives me 6, so I insert it into slot 6. 19 mod 7 gives me 5, so I insert it at 5.\n(52:53.16) The last key is 11. 11 mod 7 gives me 4, so a collision occurs at the first slot.\n(53:01.56) Linear probing is straightforward.\n(53:04.48) You simply add one.\n(53:09.32) If the first insertion is at index I, the range is from 0 to hash - 1. So the first one is +0.\n(53:18.76) Now, since a collision occurs at this location, I try to add one.\n(53:21.32) The next location will be (4 + 1) mod 7.",
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:09:15.28) Yes.\n(01:09:15.56) It's very time-consuming.\n(01:09:19.64) Yes.\n(01:09:20.08) So we try not to do it too frequently.\n(01:09:22.96) When we want to increase the size, we should double the size of the table.\n(01:09:29.20) OK, I think today I have overrun a bit.\n(01:09:34.28) Sorry about that.\n(01:09:35.32) Today, I just covered the hash table.\n(01:09:39.56) You need to know that to implement a hash table, we want to improve the search time from linear to logarithmic to constant time.\n(01:09:49.72) However, to achieve this, you need to sacrifice space.\n(01:09:53.24) You need to use more space and create a very large hash table to store the data.\n(01:09:59.60) There are a few methods: open addressing and closed addressing.\n(01:10:04.32) We introduced closed addressing, which is simple.\n(01:10:06.92) Each slot is attached to a linked list.\n(01:10:12.12) When there is a collision, meaning two keys map to the same slot, you just store them in the linked list.\n(01:10:20.28) Such closed hashing will give you a logarithmic factor.\n(01:10:26.96) It can be more than one.\n(01:10:29.24) Then there is open addressing.\n(01:10:31.64) Open addressing means if a slot is already occupied and a collision occurs, you simply use linear probing, quadratic probing, or double hashing to find an alternate location.\n(01:10:47.48) If the alternate location is occupied, you find another alternate location.\n(01:10:52.56) You will keep searching every possible slot in the hash table until you reach an empty slot.\n(01:11:00.16) Because of this, you need to be careful when deleting keys under open addressing. When you delete a key, you cannot simply leave it empty because when you search, you will miss the key.\n(01:11:17.08) So when we delete a key, we don't actually delete it.\n(01:11:31.92) However, when you search, you cannot stop at this deleted key.\n(01:11:36.40) You have to continue searching because there may still be keys behind it.\n(01:11:45.20) Yes, the last topic I will discuss today is rehashing.\n(01:11:50.36) When your hash table grows to a certain point, particularly in open addressing, when you reach a certain threshold, let's say when \u03b1 is more than 0.5,",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (53:28.84) If a collision occurs again, I simply add 1, then 2, then 3.\n(53:38.68) You will go to the next location, which is 6, but there is still a collision.\n(53:42.60) Then you go to (6 + 3), which gives you 0.\n(53:46.92) This location is empty, so you insert it there.\n(53:49.60) This is linear probing.\n(53:51.76) However, you will soon notice that in this example, if you keep finding the next location by adding 1, you will eventually reach H - 1.\n(54:10.72) If H - 1 is reached and you still cannot find any empty slot, it means this table is already full.\n(54:21.76) It is pointless to continue, as you will just repeat the procedure.\n(54:25.48) Linear probing can lead to problems because if your key hashes to the same location, you will keep moving to adjacent locations.\n(54:47.76) If this slot is occupied, you will move to the next slot, and so on.\n(55:02.04) This will form a cluster, making it take a lot of time to search because all your keys will be clustered together.\n(55:15.28) This is not a very good idea.\n(55:23.64) Therefore, someone came up with a better solution, which is quadratic probing. Instead of simply adding one, they use a quadratic function.\n(55:32.88) This way, they do not always look for the adjacent slot.\n(55:41.20) For example, if I let my hash function H equal to 2^3, I make it as 8 slots, then C1 and C2 are set to half.",
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:04:00.60) I need to choose the prime number here because I need to ensure my probe can visit every alternate slot or the possible alternate slot.\n(01:04:13.52) The time complexity for linear probing and double hashing is just for your information.\n(01:04:20.04) I will not prove it.\n(01:04:22.80) You also don't have to know how to prove it, but if you want to know how to prove it, you can check this particular book by this author.\n(01:04:30.96) This is a very well-known author.\n(01:04:35.16) Yes, he's still around.\n(01:04:36.64) He developed this concept in 1973.\n(01:04:40.36) This is quite a well-known book in computer science.\n(01:04:43.20) You can go and read it.\n(01:04:44.32) The key point I want you to note is that all this time complexity is a function of 1 / (1 - \u03b1).\n(01:04:54.56) What does it mean when your \u03b1 is close to 1?\n(01:04:57.88) For open addressing, your \u03b1 must be less than or equal to 1, right?\n(01:05:07.16) So when you are close to 1, your time complexity will grow very fast and become very high.\n(01:05:14.24) Yes, when your \u03b1 is more than 1.\n(01:05:16.84) So we need to keep \u03b1 as low as possible.\n(01:05:21.76) Usually, we try not to exceed 50% of the slots.\n(01:05:26.32) Yes.\n(01:05:26.80) This way, your search time can remain manageable.\n(01:05:33.56) The next topic is the delete key.\n(01:05:37.72) Deleting a key is not that simple because for open addressing, while for closed addressing, it is quite easy.\n(01:05:44.32) You can simply delete it without any problem, just like deleting a key from a linked list. However, deleting a key in open addressing requires caution because if you delete the key without marking it,\n(01:05:52.32) when you search for it, you may miss it because the slot appears empty, but there could be another key in the subsequent slots.\n(01:06:11.80) If you leave it as an empty slot, you won't be able to search for the subsequent key behind it.\n(01:06:26.40) So instead of actually deleting the key, we don't do that.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning-Middle\n[OCR] 14, 13, 11, 12, 10, 21, 24, 19, 15, 16, 29, 27, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . The key is squared and the middle part of the result is used as the hash, address, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 -> H(k) = 406, \u00b7 Pseudo-random number generator, \u00b7 H(k) = (a x k) mod h, 10 k -, 1 14 27, 1 2 3, 21 3, 14 15, 8 2, 13 2, 3 16, . Shift folding: Divide the key into a few parts and added up these parts, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 > H(k) = 406, 3 16 29 11 24, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing\n[Transcript] (31:02.96) In the lecture, we will discuss all the examples.\n(31:06.92) Usually, we will use the first method, modular arithmetic.\n(31:10.36) We simply mod the hash table size to map the key to a particular index.\n(31:18.20) This is the choice of the hash function.\n(31:23.28) As I mentioned, there are collisions.\n(31:26.60) No matter how you do it, there is always a chance that two different keys will map to the same slot through your hash function, which is called a collision.\n(31:37.84) But how do you resolve the collision?",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[OCR] 84, 39, 92, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing, Closed Address Hashing: Separate Chaining, \u00b7 Keys are not stored in the table itself, \u00b7 All the keys with the same hash address are store in a separate list, . During searching, the searched element with hash address i is compared with elements in linked, list H[i] sequentially, \u00b7 In closed address hashing, there will be a number of elements in each linked list on average., h(84) = h(39) = h(92) = i, Lo, uan Ren, e Yuan Ren (yrloke@ntu.edu.sg) Lo\n[Transcript] (31:40.56) Here, there are two options. There are more, but we will only introduce two. One is closed addressing hashing, also known as separate chaining.\n(31:51.80) The idea is quite simple. Since they map to the same location, why not store all the keys that map to the same location in a linked list and store the head pointer of the linked list in the hash table?\n(32:16.48) When you search for a key, your hash function brings you to this location. At this location, there is a head pointer that leads you to the linked list.\n(32:30.08) When you reach the linked list, you search along it to check whether the key is present.\n(32:39.88) This method is called closed addressing hashing.\n(32:45.28) This means your address is fixed. If you give me the key, I always use the same hash function to go to this address to search for the key.\n(33:01.04) In short, the key is not stored in the table itself; we only store the linked list.\n(33:13.04) Each hash slot comes with a linked list at the back.\n(33:19.80) All keys with the same hash address are stored in a separate list."
        ],
        "answer": "In Lecture4_Sc1007, linear probing is a method used in open addressing for resolving collisions in a hash table. When a collision occurs (i.e., when two keys hash to the same index), linear probing involves searching for the next available slot by sequentially checking the subsequent indices. Specifically, if the first insertion is at index \\( I \\), the next index to check would be \\( (I + 1) \\mod H \\), where \\( H \\) is the size of the hash table. This process continues until an empty slot is found.\n\nFor example, if you attempt to insert a key that hashes to an index that is already occupied, you would check the next index, and if that is also occupied, you would check the next one, and so on, until you find an empty slot. This method is straightforward but can lead to clustering, where a group of occupied slots forms, making searches slower. This is discussed in detail at the timestamps of **53:01.56** and **55:15.28**. \n\nOverall, linear probing is a simple yet effective way to handle collisions in hash tables, but it requires careful management to avoid performance degradation due to clustering.",
        "context_precision": 0.9266666666481335,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9232799002294669,
        "question_type": "SINGLE_DOC",
        "time_taken": 59.67749524116516,
        "question_index": 14
    },
    {
        "question": "What is secondary clustering in quadratic probing talked about in Lecture4_Sc1007?",
        "ground_truth": "Keys with the same initial hash follow the same probe sequence, causing clustering. (mentioned in 0:58:25)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (56:03.64) The movement is like +1, +3, +6, +2, +7, +5, then +4.\n(56:14.48) They do not follow a sequence like +1, +2, +3, +4.\n(56:20.08) This idea is good, but there are a few things you need to be careful about.\n(56:30.32) You must choose the number of slots carefully, as well as C1, C2, and H. If you do not select them carefully, you may not be able to visit every slot with this quadratic probing.\n(56:57.88) Ideally, I want to find alternate slots.\n(57:02.92) I need to ensure that this probing allows me to try all possible slots.\n(57:18.16) If you do not select carefully, you may find that you can only search half of the slots.\n(57:32.88) The other half may be unreachable, and once you reach half, you will start repeating the pattern.\n(57:43.20) This will be a problem in quadratic probing.\n(57:52.40) Even with quadratic probing, you will soon find that although you move in a non-sequential manner (1, 3, 6, 2, 7, 5, 4), you will still end up with a fixed pattern.\n(58:09.32) If it is a fixed pattern, your keys will always be inserted in this particular order.\n(58:14.60) When you search for a key, you will always move in this pattern, leading to another kind of clustering, known as secondary clustering.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (53:28.84) If a collision occurs again, I simply add 1, then 2, then 3.\n(53:38.68) You will go to the next location, which is 6, but there is still a collision.\n(53:42.60) Then you go to (6 + 3), which gives you 0.\n(53:46.92) This location is empty, so you insert it there.\n(53:49.60) This is linear probing.\n(53:51.76) However, you will soon notice that in this example, if you keep finding the next location by adding 1, you will eventually reach H - 1.\n(54:10.72) If H - 1 is reached and you still cannot find any empty slot, it means this table is already full.\n(54:21.76) It is pointless to continue, as you will just repeat the procedure.\n(54:25.48) Linear probing can lead to problems because if your key hashes to the same location, you will keep moving to adjacent locations.\n(54:47.76) If this slot is occupied, you will move to the next slot, and so on.\n(55:02.04) This will form a cluster, making it take a lot of time to search because all your keys will be clustered together.\n(55:15.28) This is not a very good idea.\n(55:23.64) Therefore, someone came up with a better solution, which is quadratic probing. Instead of simply adding one, they use a quadratic function.\n(55:32.88) This way, they do not always look for the adjacent slot.\n(55:41.20) For example, if I let my hash function H equal to 2^3, I make it as 8 slots, then C1 and C2 are set to half.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning-Middle\n[OCR] 14, 13, 11, 12, 10, 21, 24, 19, 15, 16, 29, 27, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . The key is squared and the middle part of the result is used as the hash, address, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 -> H(k) = 406, \u00b7 Pseudo-random number generator, \u00b7 H(k) = (a x k) mod h, 10 k -, 1 14 27, 1 2 3, 21 3, 14 15, 8 2, 13 2, 3 16, . Shift folding: Divide the key into a few parts and added up these parts, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 > H(k) = 406, 3 16 29 11 24, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing\n[Transcript] (31:02.96) In the lecture, we will discuss all the examples.\n(31:06.92) Usually, we will use the first method, modular arithmetic.\n(31:10.36) We simply mod the hash table size to map the key to a particular index.\n(31:18.20) This is the choice of the hash function.\n(31:23.28) As I mentioned, there are collisions.\n(31:26.60) No matter how you do it, there is always a chance that two different keys will map to the same slot through your hash function, which is called a collision.\n(31:37.84) But how do you resolve the collision?",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts, convenient way\n[Transcript] (27:13.80) To avoid this, we usually choose the size of the table to be a prime number.\n(27:48.36) This prime number should not be too close to any power of 2 to avoid clustering.\n(27:58.76) All the data clusters in certain locations, leading to collisions.\n(28:03.56) Even if you design a very nice hash function, if your data distribution is not random, all the data will try to map to the same place.\n(28:15.32) This is something you need to take note of when using modular arithmetic with real data.\n(28:22.60) Other types of hash functions you can choose include partitioning your key into several parts, performing some calculations, and then combining them.\n(28:35.04) Some people may shift, divide the key, and then add it back.\n(28:40.52) Typically, if your input key is a string, such as a word or a sentence, you want to store that raw data.\n(28:50.80) You may consider every alphabet in ASCII.\n(28:55.12) Then you can break it into several parts, sum them up, and come up with a value.\n(29:04.96) This particular word or sentence will be stored in a specific hash slot.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle-End\n[Transcript] (01:01:04.24) Then 13 will map to 6, and 19 will map to 5.\n(01:01:11.44) For 11, a collision occurs. 11 mod 7 gives me 3, so I add 1, and I need to move 3 steps to find the next available slot.\n(01:01:34.76) This is the next location.\n(01:01:53.52) This is double hashing.\n(01:01:55.76) You will find that every key will get a different kind of probe to look for alternate slots.\n(01:02:02.72) There will not be a fixed position, making it less likely to form clusters.\n(01:02:15.56) Your search time will not become linear.\n(01:02:22.20) This is why people prefer double hashing.\n(01:02:27.72) One thing to note is that you cannot arbitrarily select this hash function.\n(01:02:35.92) Think about why I need to add 1.\n(01:02:41.08) If I do not add 1, there is a chance that mod 3 will give you 0, causing you to stay in the same location.\n(01:03:02.44) You need to ensure that this hash function does not give you 0, as you will remain in the same location.\n(01:03:18.68) You also need to choose a prime number to ensure that the number you use is not a factor of the slot size.\n(01:03:29.44) Otherwise, you will not be able to visit every slot in a particular sequence, and you need to ensure that you can visit all possible slots.\n(01:03:49.60) So that's why they need to choose the prime number.\n(01:03:53.24) Yes.\n(01:03:53.44) The reason I need to choose the prime number earlier is due to the real data.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning\n[OCR] 4., 3., 1., ge, Dntu.edu.sg) Loke Yuan Ren (yrloke, ER SCIE, OF, Hash Functions, . Must map all possible value within the range of the hash table uniquely, \u00b7 Mapping should achieve an even distribution of the keys, \u00b7 Easy and fast to compute, 1. Modulo Arithmetic, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 5. Etc., 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . Shift folding: Divide the key into a few parts and andled up these parts\n[Transcript] (23:42.28) It should not depend on the problem size.\n(23:45.76) This means that whatever key or value you give to the hash function, it should complete in constant time.\n(24:00.56) Additionally, you need to minimize collisions.\n(24:04.32) You should try to avoid having two different keys map to the same index.\n(24:18.40) However, you cannot completely avoid collisions because the number of data items is usually much greater than the size of your hash table.\n(24:30.92) Eventually, some keys will return the same value through your hash function, resulting in a collision.\n(24:44.16) We can only try to minimize it, but we cannot avoid it entirely.\n(24:52.32) Therefore, there are a few examples of hash functions.\n(24:59.96) Typical ones include modular, arithmetic folding, mid-square, and multiplicative congruential methods, among others.\n(25:09.20) There are many methods for you to choose from when designing your hash function.\n(25:16.76) The typical one is the first one, modular arithmetic."
        ],
        "answer": "Secondary clustering in quadratic probing refers to a situation where, despite using a non-sequential probing method, the keys that are inserted into the hash table will always follow a fixed pattern when searching for a key. This fixed pattern leads to a predictable sequence of slots being probed, which can result in clustering of keys in certain areas of the hash table. \n\nThis issue arises because if the keys are inserted in a specific order, the search for a key will also follow that same order, causing multiple keys to cluster together in the same slots. This phenomenon is described in the lecture as leading to another kind of clustering, known as secondary clustering, which can negatively impact the efficiency of the hash table operations. \n\nThis concept is covered at [58:14].",
        "context_precision": 0.9166666666361111,
        "context_recall": 0.5,
        "faithfulness_result": 0.75,
        "response_relevancy": 0.9644336109494059,
        "question_type": "SINGLE_DOC",
        "time_taken": 59.01771330833435,
        "question_index": 15
    },
    {
        "question": "In Lecture4_Sc1007, why is double hashing preferred over quadratic probing?",
        "ground_truth": "It varies the probe sequence per key, reducing clustering. (mentioned in 1:02:15)",
        "context": [
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (58:25.32) If two keys have the same initial hash, they will definitely follow the same sequence (1, 3, 6, 2, 7, 5, 4).\n(58:34.12) This means your search time will increase because you will always move in the same pattern.\n(58:47.32) Even with different keys, you will still have to go through (1, 3, 6, 2, 7, 5, 4).\n(58:54.84) For example, if I insert 4, I will insert it into this location mod 8.\n(59:03.88) Then 13 mod 8 gives me 5, so I will insert it here.\n(59:11.24) For 19, mod 8 gives me 3, and for 11, there is a collision. 11 mod 8 gives me 3, so I add 1, and then I will be here.\n(59:38.04) But you will soon realize that in this example, it may not be obvious, but if you try larger data, you will find that you always move in (1, 3, 6, 2, 7, 5, 4).\n(59:49.40) Whatever key hashes to the same initial location will have to go through (1, 3, 6, 2, 7, 5, 4).\n(59:57.64) This will increase your search time.\n(01:00:03.64) Since this depends on my key, there is double hashing.\n(01:00:12.64) In double hashing, the probing depends on another hash function, which decides the number of steps I need to move.\n(01:00:22.24) This makes the probing somewhat random, not exactly random, but more varied.\n(01:00:32.20) You need to ensure that your hash table size must be a prime number.\n(01:00:41.20) In this example, if I try to insert 4, 13, and 19, if 4 mod 7 gives me 0, it maps to 0 initially with no collision.",
            "[Video title] Lecture4_Sc1007\n[Tags] Beginning-Middle\n[OCR] 14, 13, 11, 12, 10, 21, 24, 19, 15, 16, 29, 27, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Hash Functions, 2. Folding, 3. Mid-square, 4. Multiplicative Congruential Method, 1. Modulo Arithmetic: H(k) = k mod h, For keys are chosen from, \u00b7 decimal number > h avoid to use powers of 10, \u00b7 unknown lower p-bit patterns > h avoid to use powers of 2, \u00b7 \"real\" data > h should be a prime number but not too close to any power of 2, . Partition the key into several parts and combine the parts in a convenient way, . The key is squared and the middle part of the result is used as the hash, address, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 -> H(k) = 406, \u00b7 Pseudo-random number generator, \u00b7 H(k) = (a x k) mod h, 10 k -, 1 14 27, 1 2 3, 21 3, 14 15, 8 2, 13 2, 3 16, . Shift folding: Divide the key into a few parts and added up these parts, \u00b7 Eg. k=3121, k2 = 31212 = 9740641 > H(k) = 406, 3 16 29 11 24, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing\n[Transcript] (31:02.96) In the lecture, we will discuss all the examples.\n(31:06.92) Usually, we will use the first method, modular arithmetic.\n(31:10.36) We simply mod the hash table size to map the key to a particular index.\n(31:18.20) This is the choice of the hash function.\n(31:23.28) As I mentioned, there are collisions.\n(31:26.60) No matter how you do it, there is always a chance that two different keys will map to the same slot through your hash function, which is called a collision.\n(31:37.84) But how do you resolve the collision?",
            "[Video title] Lecture4_Sc1007\n[Tags] End\n[Transcript] (01:09:15.28) Yes.\n(01:09:15.56) It's very time-consuming.\n(01:09:19.64) Yes.\n(01:09:20.08) So we try not to do it too frequently.\n(01:09:22.96) When we want to increase the size, we should double the size of the table.\n(01:09:29.20) OK, I think today I have overrun a bit.\n(01:09:34.28) Sorry about that.\n(01:09:35.32) Today, I just covered the hash table.\n(01:09:39.56) You need to know that to implement a hash table, we want to improve the search time from linear to logarithmic to constant time.\n(01:09:49.72) However, to achieve this, you need to sacrifice space.\n(01:09:53.24) You need to use more space and create a very large hash table to store the data.\n(01:09:59.60) There are a few methods: open addressing and closed addressing.\n(01:10:04.32) We introduced closed addressing, which is simple.\n(01:10:06.92) Each slot is attached to a linked list.\n(01:10:12.12) When there is a collision, meaning two keys map to the same slot, you just store them in the linked list.\n(01:10:20.28) Such closed hashing will give you a logarithmic factor.\n(01:10:26.96) It can be more than one.\n(01:10:29.24) Then there is open addressing.\n(01:10:31.64) Open addressing means if a slot is already occupied and a collision occurs, you simply use linear probing, quadratic probing, or double hashing to find an alternate location.\n(01:10:47.48) If the alternate location is occupied, you find another alternate location.\n(01:10:52.56) You will keep searching every possible slot in the hash table until you reach an empty slot.\n(01:11:00.16) Because of this, you need to be careful when deleting keys under open addressing. When you delete a key, you cannot simply leave it empty because when you search, you will miss the key.\n(01:11:17.08) So when we delete a key, we don't actually delete it.\n(01:11:31.92) However, when you search, you cannot stop at this deleted key.\n(01:11:36.40) You have to continue searching because there may still be keys behind it.\n(01:11:45.20) Yes, the last topic I will discuss today is rehashing.\n(01:11:50.36) When your hash table grows to a certain point, particularly in open addressing, when you reach a certain threshold, let's say when \u03b1 is more than 0.5,",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle-End\n[Transcript] (01:01:04.24) Then 13 will map to 6, and 19 will map to 5.\n(01:01:11.44) For 11, a collision occurs. 11 mod 7 gives me 3, so I add 1, and I need to move 3 steps to find the next available slot.\n(01:01:34.76) This is the next location.\n(01:01:53.52) This is double hashing.\n(01:01:55.76) You will find that every key will get a different kind of probe to look for alternate slots.\n(01:02:02.72) There will not be a fixed position, making it less likely to form clusters.\n(01:02:15.56) Your search time will not become linear.\n(01:02:22.20) This is why people prefer double hashing.\n(01:02:27.72) One thing to note is that you cannot arbitrarily select this hash function.\n(01:02:35.92) Think about why I need to add 1.\n(01:02:41.08) If I do not add 1, there is a chance that mod 3 will give you 0, causing you to stay in the same location.\n(01:03:02.44) You need to ensure that this hash function does not give you 0, as you will remain in the same location.\n(01:03:18.68) You also need to choose a prime number to ensure that the number you use is not a factor of the slot size.\n(01:03:29.44) Otherwise, you will not be able to visit every slot in a particular sequence, and you need to ensure that you can visit all possible slots.\n(01:03:49.60) So that's why they need to choose the prime number.\n(01:03:53.24) Yes.\n(01:03:53.44) The reason I need to choose the prime number earlier is due to the real data.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[Transcript] (53:28.84) If a collision occurs again, I simply add 1, then 2, then 3.\n(53:38.68) You will go to the next location, which is 6, but there is still a collision.\n(53:42.60) Then you go to (6 + 3), which gives you 0.\n(53:46.92) This location is empty, so you insert it there.\n(53:49.60) This is linear probing.\n(53:51.76) However, you will soon notice that in this example, if you keep finding the next location by adding 1, you will eventually reach H - 1.\n(54:10.72) If H - 1 is reached and you still cannot find any empty slot, it means this table is already full.\n(54:21.76) It is pointless to continue, as you will just repeat the procedure.\n(54:25.48) Linear probing can lead to problems because if your key hashes to the same location, you will keep moving to adjacent locations.\n(54:47.76) If this slot is occupied, you will move to the next slot, and so on.\n(55:02.04) This will form a cluster, making it take a lot of time to search because all your keys will be clustered together.\n(55:15.28) This is not a very good idea.\n(55:23.64) Therefore, someone came up with a better solution, which is quadratic probing. Instead of simply adding one, they use a quadratic function.\n(55:32.88) This way, they do not always look for the adjacent slot.\n(55:41.20) For example, if I let my hash function H equal to 2^3, I make it as 8 slots, then C1 and C2 are set to half.",
            "[Video title] Lecture4_Sc1007\n[Tags] Middle\n[OCR] 84, 39, 92, ge, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, OF, Collision Resolutions, \u00b7 Closed Addressing Hashing - a.k.a separate chaining, \u00b7 Open Addressing Hashing, \u00b7 Linear Probing, \u00b7 Quadratic Probing, \u00b7 Double Probing, Closed Address Hashing: Separate Chaining, \u00b7 Keys are not stored in the table itself, \u00b7 All the keys with the same hash address are store in a separate list, . During searching, the searched element with hash address i is compared with elements in linked, list H[i] sequentially, \u00b7 In closed address hashing, there will be a number of elements in each linked list on average., h(84) = h(39) = h(92) = i, Lo, uan Ren, e Yuan Ren (yrloke@ntu.edu.sg) Lo\n[Transcript] (31:40.56) Here, there are two options. There are more, but we will only introduce two. One is closed addressing hashing, also known as separate chaining.\n(31:51.80) The idea is quite simple. Since they map to the same location, why not store all the keys that map to the same location in a linked list and store the head pointer of the linked list in the hash table?\n(32:16.48) When you search for a key, your hash function brings you to this location. At this location, there is a head pointer that leads you to the linked list.\n(32:30.08) When you reach the linked list, you search along it to check whether the key is present.\n(32:39.88) This method is called closed addressing hashing.\n(32:45.28) This means your address is fixed. If you give me the key, I always use the same hash function to go to this address to search for the key.\n(33:01.04) In short, the key is not stored in the table itself; we only store the linked list.\n(33:13.04) Each hash slot comes with a linked list at the back.\n(33:19.80) All keys with the same hash address are stored in a separate list."
        ],
        "answer": "In Lecture4_Sc1007, double hashing is preferred over quadratic probing because it provides a more varied probing sequence, which reduces the likelihood of forming clusters in the hash table. This is important because clustering can lead to increased search times, making the search process less efficient. Double hashing ensures that every key will get a different kind of probe to look for alternate slots, which helps maintain a more uniform distribution of keys across the hash table. As mentioned, \"Your search time will not become linear\" with double hashing, making it a more efficient choice for collision resolution compared to quadratic probing, which can still lead to clustering issues. This is covered at [01:02:15.56].",
        "context_precision": 0.7499999999625,
        "context_recall": 0.5,
        "faithfulness_result": 0.8,
        "response_relevancy": 0.9897717248984484,
        "question_type": "SINGLE_DOC",
        "time_taken": 59.345619916915894,
        "question_index": 16
    },
    {
        "question": "In Lecture5_Sc1007, what topics are covered in this lecture?",
        "ground_truth": "Graph representation, specifically adjacency matrix and adjacency list. (mentioned in 0:09:48)",
        "context": [
            "[Video title] Lecture5_Sc1007\n[Tags] Middle\n[OCR] ge, OF, TER SCIEN, Loke Yuan Ren, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {stations}, \u00b7 V = {atoms}, V = {computers}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/, :@ntu.edu.sg) Loke Yuan Ren (yrlo, Electrical circuits, V = {electrical devices}, E = {linkage between, devices}\n[Transcript] (27:44.44) Then you can search for the shortest or cheapest cost to travel from A to B. Yes, that is a typical graph problem.\n(27:48.40) I think more related to you is computer networks.\n(27:52.12) Maybe later you will learn about this in 2008, if I'm not wrong.\n(27:57.16) In computer networks, you can imagine that we have many computers, and all these computers connect to a switch or a router, forming the whole network.\n(28:09.56) When you want to transfer your data, your package, or your email from one point to another, you need to go through how many switches and routers to reach the destination.\n(28:23.56) That is how the routers and switches know the best path for you.\n(28:31.16) They involve some graph algorithms implemented in the switches and routers, even if you don't know it.\n(28:53.88) If you study computer engineering, those from the electrical engineering background may know about electrical circuits.\n(28:56.12) You have many components, and you need to connect all the components together.\n(29:03.80) Eventually, you need to fit all these into a PCB board, as shown in this diagram.\n(29:10.68) It's an optimization problem.",
            "[Video title] Lecture5_Sc1007\n[Tags] Middle\n[OCR] 11, 10, 12, ge, OF, TER SCIEN, Loke Yuan Ren, Dntu.edu.sg) Loke Yuan Ren (yrloke, Loke, \u00b7 Adjacency Matrix, \u00b7 Adjacency List, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {stations}, \u00b7 V = {atoms}, V = {computers}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/, Electrical circuits, V = {electrical devices}, E = {linkage between, devices}, @ntu.edu.sg) Loke Yuan Ren (yrlol, Graph Representation, Yu\n[Transcript] (29:12.16) How do you optimize it so that you can fit all the components and connections in this particular area?\n(29:24.08) Yes.\n(29:24.48) So it's another graph problem.\n(29:27.68) Many daily life problems can be formulated into graphs, and you will find that they can actually be solved.\n(29:34.96) At first glance, you may find it very difficult, but once formulated into a graph, many existing algorithms can solve it.\n(29:44.44) We may introduce some of them at this level.\n(29:48.92) In 2101, I think they will discuss more about graphs.\n(29:56.80) The first thing is how to represent the graph.\n(30:01.28) To solve the problem, you first need to be able to represent the graph properly.\n(30:05.12) You need the data.\n(30:10.88) These are the data.\n(30:11.76) This is the graph.\n(30:12.80) How do I represent it?\n(30:22.04) There are two common representations in graphs: the adjacency matrix and the adjacency list.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 25, 26, 27, 28, 29, LY, AutoSave Off, O Record, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Acrobat, X Cut, Shape Outline, Create and Share, Dictate, Sensitivity, Designer, Paste, New, Adobe PDF, Format Painter, Shape Effects, Slides, Paragraph, Drawing, Editing, Adobe Acrobat, Voice, Font, Undo, Clipboard, Open Addressing, Delete A Key Under Open Addressing, . Leave the deleted key in the table, \u00b7 Make a marker indicating that it is deleted, \u00b7 Overwrite it when a new key is inserted to the slot, ge, \u00b7 May need to do a \"garbage collection\" when a large number of, deletions are done, \u00b7 To improve the search time, Slide 27 of 29, English (Singapore), ENG, Q Search, US, Partly sunny, OF, TER SCIEN, Arrange Quick, Rehashing: Expanding the Hash Table, Summary, Replace, View, Help, Styles, Notes, ntu.edu.sg) Loke Yuan Ren (yrloke, Loke Yuan Ren (Dr), Revie, Se, ProgramLogs Microsoft, Edge, Microsoft, Word, PowerPoint, Excel, Firefox, Adobe, Insert Draw Design, Zoom, OBS Studio, Teams, IDE, FlyMcu, Tera Term, - 11, WinEdt 10, CodeBlocks Visual Studio, CodeLite, \u526a\u6620\u4e13\u4e1a\u7248, Kdenlive, Recycle Bin, Extreme, Code, Lecture_01_Introduction ToAlgo ...[Transcript] ",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] ge, OF, Summary, ntu.edu.sg) Loke Yuan Ren (yrloke, TER SCIE, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications\n[Transcript] (51:47.36) For example, if you want to find a shorter path from location A to location B,\n(51:56.20) maybe they use travel time, or maybe they use travel cost.\n(52:04.32) Yeah.\n(52:04.72) Then that depends on the problem itself.\n(52:07.76) So when using a for loop to iterate through the matrix or by looking at the linked list length, we are trying to see the degree of a particular node.\n(52:22.92) Yes, you can.\n(52:23.72) You can check the degree.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] LY, AutoSave Off, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Acrobat, Create and Share, Dictate, Sensitivity, New, Reuse, Format Painter, Shape Effects, Slides, Paragraph, Drawing, Editing, Adobe Acrobat, Clipboard, ge, English (Singapore), Accessibility: Investigate, Q Search, Partly sunny, OF, TER SCIEN, Replace, View, Help, Loke Yuan Ren, Notes, ntu.edu.sg) Loke Yuan Ren (yrloke, Loke Yuan Ren (Dr), Revie, Arrange Quic, Data Structures and, Algorithms, Graph, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Slide 1 of 32, Floyd, stack, ecnease, uct, Selection Decrease, Depth, Tree, Ren (yrloke@ntu.edu.sg) Loke Yua, Overview, \u00b7 Graph Terminology, \u00b7 Adjacency Matrix, \u00b7 Adjacency List, \u00b7 Traversal of Graphs, \u00b7 Breadth-first Search\n[Transcript] (09:04.64) Today, I will introduce the graph.\n(09:08.80) I will cover the graph and how to represent it.\n(09:16.20) You'll find that a graph cannot be stored in the program like a binary tree or a linked list.\n(09:21.96) A lot of the time, you don't have to do so.\n(09:35.08) We will introduce two representations: one is the adjacency matrix, and the other is the adjacency list.\n(09:43.32) Today, I will only cover these two.\n(09:46.84) If time allows, I will cover breadth-first search.\n(09:49.16) If not, I will cover breadth-first search and depth-first search on Wednesday.\n(09:55.20) After I finish this portion, maybe this Wednesday or next Monday, I will release the next assignment.\n(10:03.88) So please try to start working on your assignment. At least you should log in, look at the questions, and start trying out a little bit. Take notes. The assignment invitation link will only last for two weeks.\n(10:22.72) You can ask me for the access.",
            "[Video title] Lecture5_Sc1007\n[Tags] Middle\n[OCR] ge, OF, TER SCIEN, oke Yua, itu.edu.sg) Loke Yuan Ren (yrloke@, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {stations}, \u00b7 V = {atoms}, V = {computers}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/, :@ntu.edu.sg) Loke Yuan Ren (yrlo\n[Transcript] (26:25.48) For example, if you want to search for the shortest path from the source to the destination,\n(26:34.64) or if you want to travel, nowadays our MRT stations are becoming more complex.\n(26:40.96) In the past, we only had the East-West line and the North-East line, but now we have the purple line and the town circle line.\n(26:52.16) There are so many stations.\n(26:54.20) You want to find how to travel from station A to station B.\n(26:58.88) What is the shortest path?\n(27:01.44) It's a graph problem.\n(27:02.52) You can actually formulate it as a graph problem.\n(27:05.08) If you know the travel distance or travel time between each pair of adjacent stations, then that becomes the cost of each edge.\n(27:24.88) The travel time or travel cost can be considered as a weight."
        ],
        "answer": "In Lecture5_Sc1007, the topics covered include the introduction to graphs, how to represent graphs, and specific graph representations such as the adjacency matrix and adjacency list. The lecture also discusses graph applications in real-life scenarios, such as finding the shortest path in transportation systems like MRT stations, and touches on graph algorithms that can be used to solve various problems. Additionally, there is mention of optimization problems related to electrical circuits and computer networks. \n\nKey points include:\n- Introduction to graph concepts and terminology (Covered at 09:04)\n- Representations of graphs: adjacency matrix and adjacency list (Covered at 09:35)\n- Applications of graphs in finding shortest paths and optimization problems (Covered at 26:25 and 27:44)\n- Discussion of graph algorithms and their relevance in computer networks (Covered at 28:31)\n\nOverall, the lecture emphasizes the importance of graph theory in various fields and everyday problems.",
        "context_precision": 0.5333333333155555,
        "context_recall": 0.5,
        "faithfulness_result": 0.8888888888888888,
        "response_relevancy": 0.9811840924637405,
        "question_type": "SINGLE_DOC",
        "time_taken": 35.12985110282898,
        "question_index": 17
    },
    {
        "question": "In Lecture5_Sc1007, when will the lecturer release the assignment?",
        "ground_truth": "The assignment will be released this Wednesday or next Monday, and students should start practicing early. The assignment link is valid for two weeks. (mentioned in 0:10:00)",
        "context": [
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 25, 26, 27, 28, 29, LY, AutoSave Off, O Record, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Acrobat, X Cut, Shape Outline, Create and Share, Dictate, Sensitivity, Designer, Paste, New, Adobe PDF, Format Painter, Shape Effects, Slides, Paragraph, Drawing, Editing, Adobe Acrobat, Voice, Font, Undo, Clipboard, Open Addressing, Delete A Key Under Open Addressing, . Leave the deleted key in the table, \u00b7 Make a marker indicating that it is deleted, \u00b7 Overwrite it when a new key is inserted to the slot, ge, \u00b7 May need to do a \"garbage collection\" when a large number of, deletions are done, \u00b7 To improve the search time, Slide 27 of 29, English (Singapore), ENG, Q Search, US, Partly sunny, OF, TER SCIEN, Arrange Quick, Rehashing: Expanding the Hash Table, Summary, Replace, View, Help, Styles, Notes, ntu.edu.sg) Loke Yuan Ren (yrloke, Loke Yuan Ren (Dr), Revie, Se, ProgramLogs Microsoft, Edge, Microsoft, Word, PowerPoint, Excel, Firefox, Adobe, Insert Draw Design, Zoom, OBS Studio, Teams, IDE, FlyMcu, Tera Term, - 11, WinEdt 10, CodeBlocks Visual Studio, CodeLite, \u526a\u6620\u4e13\u4e1a\u7248, Kdenlive, Recycle Bin, Extreme, Code, Lecture_01_Introduction ToAlgo ...[Transcript] ",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 25, 26, 27, 28, 29, 21, 3:30, 3:3, 3:31, Loke Yuan Ren (Dr) LY, LY, Lecture_03_HashTable . Saved to \\\\cerberus\\teaching v, AutoSave Off, O Record, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Review, View Help, Acrobat, X Cut, Shape Fill, Shape Outline, Create and Share, Dictate, Sensitivity, Designer, Paste, New, Reuse, Adobe PDF, Format Painter, Convert to SmartArt, Shape Effects, Slides, Slides Section, Paragraph, Drawing, Editing, Adobe Acrobat, Voice, Font, Undo, Clipboard, Open Addressing, Delete A Key Under Open Addressing, . Leave the deleted key in the table, \u00b7 Make a marker indicating that it is deleted, \u00b7 Overwrite it when a new key is inserted to the slot, ge, \u00b7 May need to do a \"garbage collection\" when a large number of, deletions are done, \u00b7 To improve the search time, EN, Slide 27 of 29, English (Singapore), Accessibility: Investigate, ENG, Q Search, US, 29\u2103, Partly sunny, OF, edu.sg) Loke Yuan Ren (yrloke@nt\u0131, TER SCIEN, Search, Arrange Quick, Time Complexity, Rehashing: Expanding the Hash Table, Summary, Review View Help, Replace, Styles Shape Effects, View, Help, Arrange, Quick, Styles, K Cut, Loke Yuan Ren, Dntu.edu.sg) Loke Yuan Ren (yrloke, Notes, SPINE LEVEL\n[Transcript] (00:00.04) It should be OK.\n(00:00.80) I should go live.\n(00:07.36) OK.\n(00:12.20) Or five seconds?\n(00:15.08) OK, five to six seconds delay?\n(00:28.88) So this week, I will cover the graph, and in the following week, I will cover the problem related to the graph.\n(00:35.84) Before I start this week's lecture, I think a number of students have problems with double hashing under open addressing.\n(00:48.44) Something I need to point out is that although this one only has four points, you will find that when you start deleting a key under open addressing, the problem becomes more complex.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 25, 11, 10, 12, 14, 18, ge, OF, Summary, ntu.edu.sg) Loke Yuan Ren (yrloke, \u00b7 Adjacency List, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {atoms}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/, Graph Representation, Adjacency Matrix, int vSize;, int ** AdjM;, } Graph;, \u00b7 access time for AdjM[u][v] is constant, \u00b7 when graph is sparsely connected, most of the entries in AdjM are zeros, TER SCIE, struct, int id; //or weight, Adjacency List, listnode *next;, typedef struct listnode ListNode;, \u00b7 Use an array to represent the vertices, ListNode ** AdjL;, . For each vertex, use a linked list to represent the connections to other, vertices, \u00b7 Access time for AdjM[u][v] is linear, \u00b7 Space complexity is lower, O(|V|+|E|), typedef struct graph{, Represent Weighted Graphs, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications[Transcript] (52:25.40) I mean, because the degree I mentioned in the very first or second slide, which, yeah, here is the number.",
            "[Video title] Lecture5_Sc1007\n[Tags] Middle\n[OCR] ge, OF, TER SCIEN, Loke Yuan Ren, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {stations}, \u00b7 V = {atoms}, V = {computers}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/, :@ntu.edu.sg) Loke Yuan Ren (yrlo, Electrical circuits, V = {electrical devices}, E = {linkage between, devices}\n[Transcript] (27:44.44) Then you can search for the shortest or cheapest cost to travel from A to B. Yes, that is a typical graph problem.\n(27:48.40) I think more related to you is computer networks.\n(27:52.12) Maybe later you will learn about this in 2008, if I'm not wrong.\n(27:57.16) In computer networks, you can imagine that we have many computers, and all these computers connect to a switch or a router, forming the whole network.\n(28:09.56) When you want to transfer your data, your package, or your email from one point to another, you need to go through how many switches and routers to reach the destination.\n(28:23.56) That is how the routers and switches know the best path for you.\n(28:31.16) They involve some graph algorithms implemented in the switches and routers, even if you don't know it.\n(28:53.88) If you study computer engineering, those from the electrical engineering background may know about electrical circuits.\n(28:56.12) You have many components, and you need to connect all the components together.\n(29:03.80) Eventually, you need to fit all these into a PCB board, as shown in this diagram.\n(29:10.68) It's an optimization problem.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 25, 10, 14, 18, ge, OF, Summary, SPINE LEVEL, ntu.edu.sg) Loke Yuan Ren (yrloke, TER SCIE, Represent Weighted Graphs, -> (3,10) -> (4, 14), -> (2, 6) -> (3, 18), 6 18, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications\n[Transcript] (50:03.60) OK, so in this lecture, I will introduce some graph terminology: the vertex, the edge. The graph can be directed or undirected, can be adjacent, can be weighted, or can be unweighted.\n(50:19.84) Yeah.\n(50:20.16) Then we will look for a path, we will look for cycles, and so on. OK, so then we introduce two representations of graphs: the adjacency matrix and the adjacency list.\n(50:34.96) I think when the state, I will talk about the traversal.\n(50:41.08) If you have a graph, a lot of the time the problem is that you want to visit every node once. You will find the Deadpool breadth-first search and depth-first search.\n(50:51.88) I'm going to introduce those on Wednesday and maybe next Monday.\n(50:55.72) If I cannot complete, you will find it very similar to what you have learned in the binary tree. I would say that the implementation is almost exactly the same.\n(51:05.28) You need to use the stack and queue to solve that problem because they are using the same algorithm to solve the problem.\n(51:10.68) You want to visit all the nodes once in a systematic way.\n(51:16.52) How are you going to do it?\n(51:22.08) I will talk about it on Wednesday.\n(51:23.80) OK, let me answer some questions.\n(51:26.76) For the weightage, is there a specific location we have to use, or is it up to us what weight we want?\n(51:38.16) Normally, it's given by the problem.\n(51:40.16) The weight is the cost.\n(51:43.60) OK, it depends on the problem; people will define the cost.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] ge, OF, Dntu.edu.sg) Loke Yuan Ren (yrloke, itu.edu.sg) Loke Yuan Ren (yrloke@, TER SCIE, Represent Weighted Graphs, \u00b7 In the array of adjacency lists, the weight can be stored as a data field, in each list node, \u00b7 In the adjacency matrices, the weight can be stored, . The element at the u-th row and the v-th column can be defined as:, AdjM[u][v], if (u, v) E E ., otherwise, \u00b7 Constant c can be defined as 0 (weight as capacity) or some very large, number oo (weight as cost), Loke Yu, oke Yuan Rer\n[Transcript] (42:57.32) If we want to represent a graph with weights, instead of just using 1 or 0 in the adjacency matrix,\n(43:08.88) we can introduce weights and store the weights in the matrix.\n(43:10.56) If there is no edge between U and V, then you can assign a value.\n(43:26.24) Sometimes we will use zero; sometimes we will assign a very large number.\n(43:31.24) Of course, you wouldn't really store infinity; you would store some unreasonably large number to indicate that it is not possible.\n(43:38.72) Yes, it is not possible.\n(43:41.12) For example, if you want to find the minimum cost of a path between two nodes, each edge has a cost.\n(44:00.20) Then you will introduce a very high cost for those non-connected edges.\n(44:12.48) You introduce a very large cost so that the algorithm will not select that path because it is too expensive.\n(44:22.96) So they will select the lower cost path where there is a connection.\n(44:27.20) Sometimes they may not use zero because if you put zero, you are likely to choose that one since zero is the lowest cost.\n(44:40.16) Yes, sometimes we may introduce negative costs as well.\n(44:45.28) Yes, just like most of the time, that is not the case, but certain algorithms may introduce negative costs.\n(44:52.84) Yes, it is possible depending on the problem."
        ],
        "answer": "I am unable to find an answer regarding the release of the assignment in Lecture5_Sc1007. If you have any other questions or need further clarification, feel free to ask!",
        "context_precision": 0.0,
        "context_recall": 0.0,
        "faithfulness_result": 0.5,
        "response_relevancy": 0.0,
        "question_type": "SINGLE_DOC",
        "time_taken": 33.80518865585327,
        "question_index": 18
    },
    {
        "question": "In Lecture5_Sc1007, what two components make up a graph?",
        "ground_truth": "Vertices and edges. (mentioned in 0:10:03)",
        "context": [
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] ge, OF, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, ntu.edu.sg) Loke Yuan Ren (yrloke, Overview, \u00b7 Graph Terminology, \u00b7 Adjacency Matrix, \u00b7 Adjacency List, \u00b7 Traversal of Graphs, \u00b7 Breadth-first Search, Graph Terminology, \u00b7 A graph G = (V, E) consists of two finite sets:, Loke Y, \u00b7 A set V of vertices/ nodes, \u00b7 |V| is the number of vertices, . A set E of edges/arcs/links that connect the vertices, \u00b7 | E | is the number of edges ranged from 0 to\n[Transcript] (10:25.44) Then I will know what you're trying to do.\n(10:31.52) Regarding graphs, I think you may have learned about them in discrete mathematics as well.\n(10:37.72) Basically, it consists of two components: one is the vertex, and the other is the edge, and it is a finite set. All the algorithms we talk about involve input and output, and the algorithm is in a finite set.\n(10:53.76) So there is a set of vertices and a set of edges to form the graph.\n(11:01.96) Then we use this notation to indicate the number of vertices.\n(11:07.36) If these represent a set of vertices, then this notation indicates the number of vertices.\n(11:26.44) Just like vertices can be nodes, depending on their application and the problem.\n(11:32.60) Their main name is slightly different, but they are talking about the same thing regarding the graph.\n(11:38.72) So an edge is like those links that connect the vertices.\n(11:47.76) If this E connects X and Y, then X and Y are in the set of vertices. E is the edge.\n(12:04.84) This notation represents the number of edges. It can range from zero to the number of vertices times the number of vertices minus one divided by two.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 25, 10, 14, 18, ge, OF, Summary, SPINE LEVEL, ntu.edu.sg) Loke Yuan Ren (yrloke, TER SCIE, Represent Weighted Graphs, -> (3,10) -> (4, 14), -> (2, 6) -> (3, 18), 6 18, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications\n[Transcript] (50:03.60) OK, so in this lecture, I will introduce some graph terminology: the vertex, the edge. The graph can be directed or undirected, can be adjacent, can be weighted, or can be unweighted.\n(50:19.84) Yeah.\n(50:20.16) Then we will look for a path, we will look for cycles, and so on. OK, so then we introduce two representations of graphs: the adjacency matrix and the adjacency list.\n(50:34.96) I think when the state, I will talk about the traversal.\n(50:41.08) If you have a graph, a lot of the time the problem is that you want to visit every node once. You will find the Deadpool breadth-first search and depth-first search.\n(50:51.88) I'm going to introduce those on Wednesday and maybe next Monday.\n(50:55.72) If I cannot complete, you will find it very similar to what you have learned in the binary tree. I would say that the implementation is almost exactly the same.\n(51:05.28) You need to use the stack and queue to solve that problem because they are using the same algorithm to solve the problem.\n(51:10.68) You want to visit all the nodes once in a systematic way.\n(51:16.52) How are you going to do it?\n(51:22.08) I will talk about it on Wednesday.\n(51:23.80) OK, let me answer some questions.\n(51:26.76) For the weightage, is there a specific location we have to use, or is it up to us what weight we want?\n(51:38.16) Normally, it's given by the problem.\n(51:40.16) The weight is the cost.\n(51:43.60) OK, it depends on the problem; people will define the cost.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] ge, OF, Summary, ntu.edu.sg) Loke Yuan Ren (yrloke, TER SCIE, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications\n[Transcript] (51:47.36) For example, if you want to find a shorter path from location A to location B,\n(51:56.20) maybe they use travel time, or maybe they use travel cost.\n(52:04.32) Yeah.\n(52:04.72) Then that depends on the problem itself.\n(52:07.76) So when using a for loop to iterate through the matrix or by looking at the linked list length, we are trying to see the degree of a particular node.\n(52:22.92) Yes, you can.\n(52:23.72) You can check the degree.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 10, 2., Graph Terminology, \u00b7 A graph G = (V, E) consists of two finite sets:, \u00b7 A set V of vertices/ nodes, \u00b7 |V| is the number of vertices, \u00b7 | E | is the number of edges ranged from 0 to, \u00b7 Degree of a vertex is the number of edges incident to it, \u00b7 A tree is a special graph with no cycle, Undirected Graph, Weighted Graph, \u00b7 If e = (x, y) is an edge in an undirected graph, then e is incident with x, and y; x is adjacent to y and vice versa., \u00b7 If E is unordered, then G is undirected; otherwise, G is a directed, graph., \u00b7 If e = (x, y) is an edge in a directed graph, then y can be reached from, x through one edge, so target y is adjacent to source x (but it doesn't, mean x is adjacent to y)., predecessor (except for the first vertex). |V| = | E|+1, . A cycle is a path containing at least three vertices such that the last, . An undirected graph is connected if there is a path from any vertex to, any other vertex., \u00b7 A directed graph is strongly connected if there is a path from any, vertex to any other vertex., \u00b7 A graph is cyclic if it contains one or more cycles; otherwise it is, acyclic., \u00b7 A complete graph on n vertices is a simple undirected graph that, contains exactly one edge between each pair of distinct vertices., \u00b7 A path is a sequence of distinct vertices, each adjacent to the, vertex on the path is the same as the first. |V| = |E|, \u00b7 A set E of edges/arcs/links that connect the vertices\n[Transcript] (52:34.20) It is the number.\n(52:35.68) Is it in the very beginning?\n(52:39.12) Yeah, it's the number of edges.\n(52:45.64) Then you can see if you use the adjacency matrix to check the degree, you always need to check the number of vertices.\n(52:57.48) The number of iterations is the number of vertices.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 11, 10, ge, OF, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, Graph Terminology, \u00b7 A graph G = (V, E) consists of two finite sets:, \u00b7 A set V of vertices/ nodes, \u00b7 |V| is the number of vertices, . A set E of edges/arcs/links that connect the vertices, \u00b7 | E | is the number of edges ranged from 0 to, \u00b7 Degree of a vertex is the number of edges incident to it, \u00b7 A tree is a special graph with no cycle, Undirected Graph, Yuan Rer, tu.edu.sg) Loke Yuan Ren (yrloke@, Weighted Graph\n[Transcript] (14:15.20) You can try some examples, and you will find that it definitely is V times (V - 1) divided by 2.\n(14:21.92) That is the maximum number of edges you can have.\n(14:28.20) Then when they say the degree of a vertex is the number of edges incident to it.\n(14:34.80) In this case, when they are all fully connected, all the nodes, the degree is 4; there are 4 edges incident to it.\n(14:44.48) It's just terminology.\n(14:48.12) Sometimes we will use this term.\n(14:50.96) Then you have learned about the binary tree.\n(14:53.96) A tree can be considered a special case.\n(14:57.16) It's a special graph with no cycles.\n(15:00.80) When you learn about the binary tree, the binary tree is just one example of a tree.\n(15:08.56) A tree can have many children, more than one or two.\n(15:13.64) The tree definitely wouldn't form any cycles; if any siblings are connected, then they will form a cycle.\n(15:27.92) This is not considered a tree; a tree has no cycles.\n(15:33.40) So, yes, that is a tree, but it is a special graph.\n(15:45.52) You can consider it a special case in the graph.\n(15:49.24) There are many kinds of graphs.\n(15:51.56) It can be an undirected graph, meaning all the edges have no direction.\n(15:59.76) This means A is connected to B, and you can also say B is connected to A.\n(16:03.64) You can also introduce some weight on the edges, and it becomes a weighted graph.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] LY, AutoSave Off, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Acrobat, Create and Share, Dictate, Sensitivity, New, Reuse, Format Painter, Shape Effects, Slides, Paragraph, Drawing, Editing, Adobe Acrobat, Clipboard, ge, English (Singapore), Accessibility: Investigate, Q Search, Partly sunny, OF, TER SCIEN, Replace, View, Help, Loke Yuan Ren, Notes, ntu.edu.sg) Loke Yuan Ren (yrloke, Loke Yuan Ren (Dr), Revie, Arrange Quic, Data Structures and, Algorithms, Graph, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Slide 1 of 32, Floyd, stack, ecnease, uct, Selection Decrease, Depth, Tree, Ren (yrloke@ntu.edu.sg) Loke Yua, Overview, \u00b7 Graph Terminology, \u00b7 Adjacency Matrix, \u00b7 Adjacency List, \u00b7 Traversal of Graphs, \u00b7 Breadth-first Search\n[Transcript] (09:04.64) Today, I will introduce the graph.\n(09:08.80) I will cover the graph and how to represent it.\n(09:16.20) You'll find that a graph cannot be stored in the program like a binary tree or a linked list.\n(09:21.96) A lot of the time, you don't have to do so.\n(09:35.08) We will introduce two representations: one is the adjacency matrix, and the other is the adjacency list.\n(09:43.32) Today, I will only cover these two.\n(09:46.84) If time allows, I will cover breadth-first search.\n(09:49.16) If not, I will cover breadth-first search and depth-first search on Wednesday.\n(09:55.20) After I finish this portion, maybe this Wednesday or next Monday, I will release the next assignment.\n(10:03.88) So please try to start working on your assignment. At least you should log in, look at the questions, and start trying out a little bit. Take notes. The assignment invitation link will only last for two weeks.\n(10:22.72) You can ask me for the access."
        ],
        "answer": "In Lecture5_Sc1007, a graph consists of two components: a set of vertices (or nodes) and a set of edges (or links) that connect the vertices. This is covered at [10:37.72].",
        "context_precision": 0.99999999998,
        "context_recall": 0.5,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9386158052588024,
        "question_type": "SINGLE_DOC",
        "time_taken": 63.86643123626709,
        "question_index": 19
    },
    {
        "question": "In Lecture5_Sc1007, what makes a tree a special kind of graph?",
        "ground_truth": "It has no cycles. (mentioned in 0:15:00)",
        "context": [
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 11, 10, ge, OF, TER SCIEN, Dntu.edu.sg) Loke Yuan Ren (yrloke, Graph Terminology, \u00b7 A graph G = (V, E) consists of two finite sets:, \u00b7 A set V of vertices/ nodes, \u00b7 |V| is the number of vertices, . A set E of edges/arcs/links that connect the vertices, \u00b7 | E | is the number of edges ranged from 0 to, \u00b7 Degree of a vertex is the number of edges incident to it, \u00b7 A tree is a special graph with no cycle, Undirected Graph, Yuan Rer, tu.edu.sg) Loke Yuan Ren (yrloke@, Weighted Graph\n[Transcript] (14:15.20) You can try some examples, and you will find that it definitely is V times (V - 1) divided by 2.\n(14:21.92) That is the maximum number of edges you can have.\n(14:28.20) Then when they say the degree of a vertex is the number of edges incident to it.\n(14:34.80) In this case, when they are all fully connected, all the nodes, the degree is 4; there are 4 edges incident to it.\n(14:44.48) It's just terminology.\n(14:48.12) Sometimes we will use this term.\n(14:50.96) Then you have learned about the binary tree.\n(14:53.96) A tree can be considered a special case.\n(14:57.16) It's a special graph with no cycles.\n(15:00.80) When you learn about the binary tree, the binary tree is just one example of a tree.\n(15:08.56) A tree can have many children, more than one or two.\n(15:13.64) The tree definitely wouldn't form any cycles; if any siblings are connected, then they will form a cycle.\n(15:27.92) This is not considered a tree; a tree has no cycles.\n(15:33.40) So, yes, that is a tree, but it is a special graph.\n(15:45.52) You can consider it a special case in the graph.\n(15:49.24) There are many kinds of graphs.\n(15:51.56) It can be an undirected graph, meaning all the edges have no direction.\n(15:59.76) This means A is connected to B, and you can also say B is connected to A.\n(16:03.64) You can also introduce some weight on the edges, and it becomes a weighted graph.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 10, 2., Graph Terminology, \u00b7 A graph G = (V, E) consists of two finite sets:, \u00b7 A set V of vertices/ nodes, \u00b7 |V| is the number of vertices, \u00b7 | E | is the number of edges ranged from 0 to, \u00b7 Degree of a vertex is the number of edges incident to it, \u00b7 A tree is a special graph with no cycle, Undirected Graph, Weighted Graph, \u00b7 If e = (x, y) is an edge in an undirected graph, then e is incident with x, and y; x is adjacent to y and vice versa., \u00b7 If E is unordered, then G is undirected; otherwise, G is a directed, graph., \u00b7 If e = (x, y) is an edge in a directed graph, then y can be reached from, x through one edge, so target y is adjacent to source x (but it doesn't, mean x is adjacent to y)., predecessor (except for the first vertex). |V| = | E|+1, . A cycle is a path containing at least three vertices such that the last, . An undirected graph is connected if there is a path from any vertex to, any other vertex., \u00b7 A directed graph is strongly connected if there is a path from any, vertex to any other vertex., \u00b7 A graph is cyclic if it contains one or more cycles; otherwise it is, acyclic., \u00b7 A complete graph on n vertices is a simple undirected graph that, contains exactly one edge between each pair of distinct vertices., \u00b7 A path is a sequence of distinct vertices, each adjacent to the, vertex on the path is the same as the first. |V| = |E|, \u00b7 A set E of edges/arcs/links that connect the vertices\n[Transcript] (52:34.20) It is the number.\n(52:35.68) Is it in the very beginning?\n(52:39.12) Yeah, it's the number of edges.\n(52:45.64) Then you can see if you use the adjacency matrix to check the degree, you always need to check the number of vertices.\n(52:57.48) The number of iterations is the number of vertices.",
            "[Video title] Lecture5_Sc1007\n[Tags] Middle\n[OCR] ge, OF, TER SCIEN, Graph Terminology, . An undirected graph is connected if there is a path from any vertex to, any other vertex., \u00b7 A directed graph is strongly connected if there is a path from any, vertex to any other vertex., \u00b7 A graph is cyclic if it contains one or more cycles; otherwise it is, acyclic., itu.edu.sg) Loke Yuan Ren (yrloke@, \u00b7 A complete graph on n vertices is a simple undirected graph that, contains exactly one edge between each pair of distinct vertices., LO, Graph Applications, Maps, Organic Chemistry, Computer Networks, \u00b7 V = {stations}, \u00b7 V = {atoms}, V = {computers}, \u00b7 E = {underground route}, \u00b7 E = {bonds between, E = {connections, between computers}, atoms}, Aforl. (2014). A map of Singapore's Mass Rapid Transit (MRT) and Light Rail Transit (LRT) systems [Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Singapore_MRT_and_LRT_System_Map.svg, File: Electric circuit [Image]. (2013). Retrieved from https://pixabay.com/en/board-chip-circuit-electric-158973, Chemistry-atoms [Image]. (2015). Retrieved from https://pixabay.com/en/pentanol-molecule-chemistry-atoms-867210/\n[Transcript] (24:59.20) As long as I find one cycle in the graph, then that particular graph is considered cyclic.\n(25:05.80) A tree is one where you cannot find a cycle.\n(25:09.36) For example, if you have a graph that resembles a tree structure,\n(25:16.08) you definitely cannot find any cycle.\n(25:21.20) If that is the case, then you can set this as a cyclic graph.\n(25:26.68) This one does not have a cycle; no matter how you look at it, you won't be able to find any cycle in the tree.\n(25:37.72) This one is not cyclic.\n(25:43.28) This one is cyclic, and I have already explained this to you.\n(25:49.56) A complete graph on n vertices is a simple undirected graph that contains exactly one edge between each pair of distinct vertices.\n(26:01.68) Then the total number of edges will be v * (v - 1) / 2.\n(26:11.64) This is very useful.\n(26:13.20) In many sectors, problems can be formulated into graph problems, and then algorithms can be used to solve them.",
            "[Video title] Lecture5_Sc1007\n[Tags] End\n[OCR] 25, 10, 14, 18, ge, OF, Summary, SPINE LEVEL, ntu.edu.sg) Loke Yuan Ren (yrloke, TER SCIE, Represent Weighted Graphs, -> (3,10) -> (4, 14), -> (2, 6) -> (3, 18), 6 18, \u00b7 Concepts and terminologies of graph, such as, \u00b7 A graph consists of a set of vertices and a set of edges, \u00b7 Directed vs. undirected graphs, \u00b7 The definitions of path and cycle, etc., \u00b7 Two data structures used to represent graphs:, \u00b7 Adjacency matrix, \u00b7 Array of adjacency lists, \u00b7 Their advantages and disadvantages for different applications\n[Transcript] (50:03.60) OK, so in this lecture, I will introduce some graph terminology: the vertex, the edge. The graph can be directed or undirected, can be adjacent, can be weighted, or can be unweighted.\n(50:19.84) Yeah.\n(50:20.16) Then we will look for a path, we will look for cycles, and so on. OK, so then we introduce two representations of graphs: the adjacency matrix and the adjacency list.\n(50:34.96) I think when the state, I will talk about the traversal.\n(50:41.08) If you have a graph, a lot of the time the problem is that you want to visit every node once. You will find the Deadpool breadth-first search and depth-first search.\n(50:51.88) I'm going to introduce those on Wednesday and maybe next Monday.\n(50:55.72) If I cannot complete, you will find it very similar to what you have learned in the binary tree. I would say that the implementation is almost exactly the same.\n(51:05.28) You need to use the stack and queue to solve that problem because they are using the same algorithm to solve the problem.\n(51:10.68) You want to visit all the nodes once in a systematic way.\n(51:16.52) How are you going to do it?\n(51:22.08) I will talk about it on Wednesday.\n(51:23.80) OK, let me answer some questions.\n(51:26.76) For the weightage, is there a specific location we have to use, or is it up to us what weight we want?\n(51:38.16) Normally, it's given by the problem.\n(51:40.16) The weight is the cost.\n(51:43.60) OK, it depends on the problem; people will define the cost.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] LY, AutoSave Off, File, Home, Insert, Draw, Design, Transitions, Animations, Slide Show, Record, Acrobat, Create and Share, Dictate, Sensitivity, New, Reuse, Format Painter, Shape Effects, Slides, Paragraph, Drawing, Editing, Adobe Acrobat, Clipboard, ge, English (Singapore), Accessibility: Investigate, Q Search, Partly sunny, OF, TER SCIEN, Replace, View, Help, Loke Yuan Ren, Notes, ntu.edu.sg) Loke Yuan Ren (yrloke, Loke Yuan Ren (Dr), Revie, Arrange Quic, Data Structures and, Algorithms, Graph, Science, Dr. Loke Yuan Ren, Lecturer, College of Engineering, yrloke@ntu.edu.sg, School of Computer Science and Engineering, Slide 1 of 32, Floyd, stack, ecnease, uct, Selection Decrease, Depth, Tree, Ren (yrloke@ntu.edu.sg) Loke Yua, Overview, \u00b7 Graph Terminology, \u00b7 Adjacency Matrix, \u00b7 Adjacency List, \u00b7 Traversal of Graphs, \u00b7 Breadth-first Search\n[Transcript] (09:04.64) Today, I will introduce the graph.\n(09:08.80) I will cover the graph and how to represent it.\n(09:16.20) You'll find that a graph cannot be stored in the program like a binary tree or a linked list.\n(09:21.96) A lot of the time, you don't have to do so.\n(09:35.08) We will introduce two representations: one is the adjacency matrix, and the other is the adjacency list.\n(09:43.32) Today, I will only cover these two.\n(09:46.84) If time allows, I will cover breadth-first search.\n(09:49.16) If not, I will cover breadth-first search and depth-first search on Wednesday.\n(09:55.20) After I finish this portion, maybe this Wednesday or next Monday, I will release the next assignment.\n(10:03.88) So please try to start working on your assignment. At least you should log in, look at the questions, and start trying out a little bit. Take notes. The assignment invitation link will only last for two weeks.\n(10:22.72) You can ask me for the access.",
            "[Video title] Lecture5_Sc1007\n[Tags] Beginning\n[OCR] 3., ge, OF, TER SCIEN, PINE LEVEL, Graph Terminology, Undirected Graph, tu.edu.sg) Loke Yuan Ren (yrloke@, Weighted Graph, Directed Graph, oke Yua, e@ntu.edu.sg) Loke Yuan Ren (yrlo, Complete Graph, \u00b7 If e = (x, y) is an edge in an undirected graph, then e is incident with x, and y; x is adjacent to y and vice versa., n (yrloke@ntu.edu.sg) Loke Yuan R, Ren (yrloke@ntu.edu.sg), ke@ntu.edu.sg) Loke Yuan Ren (yrl, \u00b7 If E is unordered, then G is undirected; otherwise, G is a directed, graph.\n[Transcript] (16:10.80) For example, if you want to travel from A to B, there is some cost.\n(16:18.24) I can use the weight to represent the cost.\n(16:21.96) So I can know that if I want to travel from A to B through this particular edge, the cost is 7.\n(16:29.60) If I travel from A to D to B, then the total cost is 5, which is 3 + 2.\n(16:37.60) If I travel from here to here, then it becomes 5, which is lower.\n(16:42.36) Sometimes we need to introduce the weight into the graph to assist us in solving the problem.\n(16:52.04) We can also introduce direction.\n(17:01.92) This is considered a complete graph; they are fully connected.\n(17:08.40) These are just different kinds of graphs we can have for different problems.\n(17:15.76) If E connects X and Y as an edge in an undirected graph, then E is incident with X and Y. I can say X is adjacent to Y and Y is adjacent to X.\n(17:41.28) If it is an edge of an undirected graph, then I can say X is adjacent to Y and Y is adjacent to X.\n(17:46.80) If E is unordered, then G is an undirected graph; otherwise, G is a directed graph."
        ],
        "answer": "A tree is considered a special kind of graph because it is defined as a graph that has no cycles. This means that in a tree structure, you cannot find any closed loops or cycles connecting the vertices. If any siblings in a tree are connected, it would form a cycle, which would disqualify it from being a tree. Therefore, a tree is characterized by its acyclic nature, making it a unique case within graph theory. This concept is explained in the lecture at \"Covered at 15:27\".",
        "context_precision": 0.9999999999666667,
        "context_recall": 1.0,
        "faithfulness_result": 1.0,
        "response_relevancy": 0.9527605090618229,
        "question_type": "SINGLE_DOC",
        "time_taken": 52.42116737365723,
        "question_index": 20
    }
]